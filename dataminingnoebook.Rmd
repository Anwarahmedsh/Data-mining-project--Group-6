---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

#importing dataset
```{r}
setwd("C:/Users/saran/OneDrive")
dataset = read.csv('BreastCancer1.csv')
str(dataset)
library(Hmisc)
describe(dataset)
summary(dataset)
```
# problem 
# Breast cancer, a leading cause of global cancer-related deaths, presents a challenge in tailoring treatments due to its heterogeneity in nature, but homogeneity in treatment due to the fact that the current treatment landscape often lacks personalized approaches, primarily because of a limited emphasis on prognosis assessment. This oversight becomes glaringly evident in the observed survival rate disparities, such as the approximate 90% survival rate for caucasian women compared to the 80% rate for African American women.
By focusing on prognosis, we aim to bridge the gap in treatment personalization and resource allocation. This is not merely a statistical concern but a pressing matter of healthcare equity and the optimal utilization of resources.
The goal of this project is to develop a machine learning model that goes beyond traditional diagnostic approaches and provides a nuanced understanding of individual patient prognosis. By identifying factors contributing to higher risk of death, we aspire to enhance treatment personalization, ensuring that resources are allocated more efficiently to patients who are at a greater risk. This approach not only addresses the disparities in survival rates among different demographic groups but also contributes to a more equitable and effective breast cancer treatment landscape on a global scale.

# Data Mining Task :
# Data Mining Tasks: Classification and Clustering

# This project involves two primary data mining tasks: classification and clustering, each contributing to the overarching goal of helping healthcare professionals to make informed decision, and enhancing treatment plans, improved counseling, and resource allocation towards patients who have a higher risk of death.

# 1. Classification:
#Class Attribute: The class attribute for the classification task is the "patient status," which is binary and categorized as either "alive" or "dead."

# Goal: The classification model can be used to predict whether a patient's status will be "alive" or dead". This will serve as a valuable tool for healthcare professionals to make informed decision regarding patient careuch as early intervention strategies , since these predictions can be used to identify risk factors for breast cancer death, which could result in developing more optimized strategies and treatments.

# 2. Clustering:
#Goal:  the clustering task aims to uncover patterns and subgroups within the patient population based on shared characteristics Utilizing clinical data such as T-Stage, age, and N-Stage. This enables a more nuanced understanding of the heterogeneity of breast cancer, facilitating the identification of specific patient profiles that may have similar prognostic implications.

# Data:
# The source of the dataset
# obtained from the 2017 November update of the SEER Program of the NCI, from kaggle website, URL: https://www.kaggle.com/datasets/reihanenamdari/breast-cancer

# Dataset Description
# General Information 

- Number of attributes : 16
- Number of objects : 4025 
- The class name : status.
```{r}
#Number of rows
nrow(dataset)
#Number of columns
ncol(dataset)
```
- Types of attribute :

    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | **Attribute name**     | **Description**                                                                                                                       | **Data type**     | **Possible values**                                                                       |
    +:=======================+:======================================================================================================================================+:==================+:==========================================================================================+
    | Age                    | age of the patients                                                                                                                   | numeric           | between 30 - 70                                                                           |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Race                   | classification of people based on their physical characteristics such as skin color                                                   | nominal           | white, black or other                                                                     |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Marital status         | refers to wether the patient is married ,single or divorced etc                                                                       | nominal           | Divorced ,Married, Separated, Single , Widowed                                            |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | T stage                | describe the tumors size and if it has spread to the skin or to the chest wall under the breast. Higher T numbers mean a larger tumor | ordinal           | T1,T2,T3,T4                                                                               |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | N stage                | indicates whether the cancer has spread to lymph nodes near the breast                                                                | ordinal           | N1 ,N2,N3                                                                                 |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | 6th stage              | describes invasive breast cancer                                                                                                      | Ordinal           | IIA , IIB , IIIA , IIIB , IIIC                                                            |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Differentiation        | describes how well developed the tumour cells are and how cancer cells are organised in the tumour tissue                             | ordinal           | Moderately differentiated , Poorly differentiated ,Undifferentiated , Well differentiated |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Grade                  | depends on what the cells look like, lower grade indicates slower-growing cancer unlike higher grades                                 | ordinal           | 1,2 ,3, 4                                                                                 |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | A stage                | Regional : aneoplasm that has extended, Distant : aneoplasm that has spread to parts of the body remote from                          | asymmetric binary | Distant, Regional                                                                         |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Tumor size             | Each indicates exact size of the tumor in millimeters                                                                                 | ordinal           | FROM 1 TO 140                                                                             |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Estrogen status        | The cancer is estrogen receptor-positive if it has receptors for estrogen                                                             | asymmetric binary | Negative, Positive                                                                        |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Progesterone status    | The cancer is progesterone receptor-positive if it has progesterone receptors.                                                        | asymmetric binary | Negative , Positive                                                                       |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Regional Node Examined | the number of the regional node that have been examined                                                                               | numeric           | FROM 1 TO 61                                                                              |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Reginol Node Positive  | the number of the regional node that heve been confirmed to have cancer                                                               | numeric           | FROM 1 TO 46                                                                              |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Survival Months        | It predicts the number of months to live based on the patient state                                                                   | numeric           | FROM 1 TO 107                                                                             |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+
    | Status                 | describes weather the patient is dead or alive                                                                                        | asymmetric binary | Dead , alive                                                                              |
    +------------------------+---------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------------------------------------------------------------------------------+


# Moving forward, we applied some statistical measures on the dataset to better understand them. The following code show the process:

#statistical summaries
```{r}
summary(dataset)
var(dataset)
var(dataset$Tumor.Size)
var(dataset$Regional.Node.Examined)
var(dataset$Reginol.Node.Positive)
var(dataset$Survival.Months)
var(dataset$Age)
```


# Boxplot
# here we have a graphic display of the five-number summary and the outliers
```{r}
# Select the columns that have numercal values
columns_of_interest <- c("Age", "Tumor.Size", "Regional.Node.Examined", "Reginol.Node.Positive", "Survival.Months")
data_of_interest <- dataset[, columns_of_interest]

# Create boxplots for each selected attribute
par(mfrow = c(2, 3))  # Adjust the layout as needed

for (attribute in columns_of_interest) {
  boxplot(data_of_interest[[attribute]], main = attribute)
}
```
# as illustrated in the graphs the age attrbuite have no outliers, but the Tumor.Size, Regional.Node.Examined and Reginol.Node.Positive have only Max Outlier meanwhile Survival.Months have only Min Outlier

# we have represented the dataset in the following graphs to provide better understanding:

# histogram for Age attribute
```{r}
hist(dataset$Age)
```
# A histogram showing the age distribution of women diagnosed with breast cancer. We conclude that breast cancer is more common among women whose ages range from 45 to 50 years (highest frequency). However, it illustrates the wide range of ages, which could cause computation errors and necessitate transformation to bring the ages within a more manageable range.

# pie chart for Race attribute
```{r}
library(dplyr)
tab = dataset$Race %>% table()
precentages = tab %>% prop.table() %>% round(3)*100
txt = paste0(names(tab), '\n', precentages, '%')
pie(tab, labels = txt, main= 'Race')
```
# we can see that the race is divided into three sections with their percentage ( as our dataset shows we have white, black , other). The pie chart shows which is the most Race diagnosed with breast cancer. as illustrated, the white race is the highest diagnosed race with 84.8% rate


#Scatter Plot
```{r}
with(dataset, plot(Tumor.Size, Survival.Months, col = "blue", pch = 16))
```
# This scatter plot shows the collarition between the tumor size attribute and the Survival months attribute, as when the tumor size is small the survival months is high which indicates that when the tumor size is small the women is more likely to live more months, also it illustrates there is a lot of outliers in the dataset so we might need to delete them.


# detecting outliers is crucial for data integrity, accurate analysis, model performance, and gaining a comprehensive understanding of the data. It helps identify potential data issues,also its important to detect it before starting the data preprocessing step since it can ensure that preprocessing steps are applied appropriately and that the resulting data is suitable

#here we analyzed their occurrences for specific variables of the dataset, and deleted them
#here we found that the age has outliers not like what the boxplot indctated, for better results we chose to remove them
```{r}
#install.packages("outliers")
library(outliers)
#outliers of age
outage= outlier(dataset$Age)
print(outage)
#number of rows that have the outlier 
table(dataset$Age == outage )

#outliers of tumor size
outTumorSize = outlier(dataset$Tumor.Size)
print(outTumorSize)
#number of rows that has the outlier
table(dataset$Tumor.Size == outTumorSize)

#outliers of Regional Node Examined
outNodeExamined = outlier(dataset$Regional.Node.Examined)
print(outNodeExamined)
#number of rows that has the outlier
table(dataset$Regional.Node.Examined == outNodeExamined )

#outliers of Reginol Node Positive
outNodepositive = outlier(dataset$Reginol.Node.Positive)
print(outNodepositive)
#number of rows that has the outlier
table(dataset$Reginol.Node.Positive == outNodepositive )

#outliers of Survival Months
outmonth = outlier(dataset$Survival.Months)
print(outmonth)
#number of rows that has the outlier 
table(dataset$Survival.Months==outmonth)
```
```{r}
#delete the outliers
dataset = dataset[dataset$Age != outage ,]
dataset = dataset[dataset$Tumor.Size != outTumorSize ,]
dataset = dataset[dataset$Regional.Node.Examined != outNodeExamined ,]
dataset = dataset[dataset$Reginol.Node.Positive != outNodepositive ,]
dataset = dataset[dataset$Survival.Months != outmonth ,]
nrow(dataset)
```


# Data preprocessing:

In the data preprocessing, we applied various techniques to enhance the dataset's suitability for machine learning models. We did both data cleaning and data transformation.  
For the data cleaning :
we have already removed the outliers , we checked if our dataset have duplicated rows and null values or not but it didnt have any. 
For data transformation: 
-Normalization was performed on numeric attributes to ensure a consistent scale between 0 and 1.
-Encoding was performed on categorical variables, converting them into a numeric format, facilitating model training.  
-Chi-square tests were conducted to evaluate the associations between each categorical attribute and the "Status" variable, revealing significant connections.
-Correlation analysis generated a matrix for numeric variables, to understand relationships between them.

#Data cleaning codes:
checking if there is any missing value or duplicated rows, this allow us to identify and handle data quality issues early in the analysis process
```{r}
is.na(dataset)
sum(is.na(dataset))
sum(duplicated(dataset))
#removing duplicate
dataset= unique(dataset)
sum(duplicated(dataset))
#number of rows after deleting dup
nrow(dataset)
nrow(dataset)
```
we did not have any missing valuse but we have a duplicated row so we deleted it.

#Data transformation codes:

#Normalization:
we had numeric attributes might be measured in different units or have a different range of values (Age, tumor size, Regional Node Examined, Regional Node Positive, Survival months). so we applied Normalization to adjust the quantities of each attribute so that they all fit into the same scale and they all have values between 0 and 1.
```{r}
#Define function normalize
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
#Call normalize function
dataset$Age<-normalize(dataset$Age)
dataset$Tumor.Size<-normalize(dataset$Tumor.Size)
dataset$Regional.Node.Examined<-normalize(dataset$Regional.Node.Examined)
dataset$Reginol.Node.Positive<-normalize(dataset$Reginol.Node.Positive)
dataset$Survival.Months<-normalize(dataset$Survival.Months)
#showing normalized columns
print(dataset)
```

#Encoding
The goal of this code is to convert categorical variables in the dataset into a numeric format. This process is called encoding. this making the dataset more accessible and suitable for machine learning algorithms that require numerical input for training and making predictions.
```{r}
dataset$Race = factor(dataset$Race,levels = c("White","Black", "Other"), labels=c(1,2,3))
dataset$Marital.Status = factor(dataset$Marital.Status,levels = c("Married","Divorced","Separated","Single ","Widowed"), labels=c(1,2,3,4,5))
dataset$differentiate = factor(dataset$differentiate,levels = c("Well differentiated","Moderately differentiated", "Poorly differentiated","Undifferentiated"), labels=c(1,2,3,4))
dataset$Grade= factor(dataset$Grade,levels = c(1 , 2 ,3 ," anaplastic; Grade IV"), labels=c(1,2,3,4))
dataset$X6th.Stage = factor(dataset$X6th.Stage,levels = c("IIA","IIIA", "IIB","IIIB","IIIC"), labels=c(1,2,3,4,5))
dataset$Status = factor(dataset$Status,levels = c("Dead","Alive"), labels=c(0,1))
dataset$Estrogen.Status = factor(dataset$Estrogen.Status ,levels = c("Negative","Positive"), labels=c(0,1))
dataset$Progesterone.Status = factor(dataset$Progesterone.Status ,levels = c("Negative","Positive"), labels=c(0,1))
dataset$A.Stage	 = factor(dataset$A.Stage	,levels = c("Regional","Distant"), labels=c(0,1))
dataset$T.Stage	 = factor(dataset$T.Stage	,levels = c("T1","T2","T3","T4"), labels=c(1,2,3,4))
dataset$N.Stage	 = factor(dataset$N.Stage	,levels = c("N1","N2","N3"), labels=c(1,2,3))
View(dataset)
```

```{r}
is.na(dataset)
sum(is.na(dataset))
```

#the chi-square
this code is conducting a series of chi-square tests to assess the association between each categorical attribute in the dataset and the "Status" variable.
```{r}
# Get the column names of the dataset
column_names <- colnames(dataset)

# Perform chi-square test for each attribute
for (attribute in column_names) {
  if (attribute != "Status") {
    # Create a contingency table
    contingency_table <- table(dataset[[attribute]], dataset$Status)
    
    # Perform chi-square test
    result <- chisq.test(contingency_table)
    
    # Print the attribute name and the result
    cat("Attribute:", attribute, "\n")
    print(result)
    cat("\n")
  }
}
```
the chi-square test results indicate that most of the attributes have a significant association with the variable being tested. These attributes include Age, Race, Marital.Status, T.Stage, N.Stage, X6th.Stage, differentiate, Grade, A.Stage, Estrogen.Status, Progesterone.Status, Regional.Node.Examined, Reginol.Node.Positive, Tumor.Size, Survival.Months, and Status. The low p-values provide strong evidence to reject the null hypothesis of no association between these attributes and the variable. #Since they are highly correlated there is no need to perform feature selection


#correlation analysis
This code extracts numeric attributes from the dataset, calculates the correlation matrix for those numeric variables, and then prints the correlation matrix.
```{r}
numeric_datas <- dataset[, sapply(dataset, is.numeric)]

# Calculate the correlation matrix for numeric variables
correlation_matrix <- cor(numeric_datas)

# Print the correlation matrix
print(correlation_matrix)

```
The matrix shows the correlation coefficients between pairs of variables. The values range from -1 to 1, where -1 indicates a perfect negative correlation, 1 indicates a perfect positive correlation, and 0 indicates no correlation.
For example, looking at "Tumor.Size" and "Reginol.Node.Positive," the correlation coefficient is 0.245566, indicating a positive correlation which means that the larger tumor size the greater number of nodes affected . In contrast, "Reginol.Node.Positive" and "Survival.Months" have a correlation coefficient of -0.13344498 which means the greater number of affected nodes the fewer months to live



#feature selection 
other way to choose the relevant attributes to the dataset, we performed this feature selection if you want to check it out, although we think the chi square is better \# ensure the results are repeatable

```{r}
set.seed(7)
# load the library
library(mlbench)
library(caret)
# Split the dataset into features and target variable
features <- dataset[, -c(2 , 3 ,16)]  # Exclude non-numeric and target variable columns
target <- dataset$Status

# Define the control parameters for RFE
ctrl <- rfeControl(functions = rfFuncs, 
                   method = "cv",
                   number = 10)  # 10-fold cross-validation

# Assuming 'rfFuncs' is correctly defined for Recursive Feature Elimination using Random Forest
rfe_result <- rfe(features, target, sizes = 1:ncol(features), rfeControl = ctrl)

# Print the RFE result
print(rfe_result)
plot(rfe_result)
```

This feature selection graphical representation shows the higher-ranked features that considered more important or relevant for predicting the target variable which are: Survival.Months, Reginol.Node.Positive, X6th.Stage, Progesterone. Status, Age.,this helps choosing which features to include or exclude . so we can improve model efficiency, reduce overfitting, and enhance interpretability by focusing on the most relevant features for the task at hand.


#Data mining technique

#Classification
For classification applied three different classification algorithms ID3 with information gain, CART with Gini index, and C4.5 (C5.0) with gain ratio for each size ( 70% 30% , 60% 40% , 80% 20% ). .
-For the ID3 algorithm, we utilized the rpart package with the information split method, and the resulting decision tree was visualized and evaluated using confusion matrix, sensitivity, specificity, precision, and accuracy.
-For the CART algorithm, we used the gini split method with the rpart package, visualized the decision tree, and evaluated the model's performance using confusion matrix, sensitivity, specificity, precision, and accuracy.
-For the C4.5 (C5.0) algorithm, we employed the C50 package and visualized the decision tree using the party and partykit packages, then evaluated using confusion matrix, sensitivity, specificity, precision, and accuracy.

#Tree 1
70% training and 30% and infomation gain
```{r}
dataset$Status <- factor(dataset$Status)
# Set the random seed for reproducibility
set.seed(1234)

# Split the dataset into training 70% and testing 30% subsets
ind <- sample(1:nrow(dataset), size = floor(0.7 * nrow(dataset)), replace = FALSE)
trainData <- dataset[ind,]
testData <- dataset[-ind,]
myFormula <- Status ~ Age + Marital.Status + Race + T.Stage + N.Stage + X6th.Stage + differentiate + Grade + A.Stage + Tumor.Size + Estrogen.Status + Progesterone.Status + Reginol.Node.Positive + Regional.Node.Examined + Survival.Months 
```

```{r}
# Load the necessary library
install.packages('party')
library(party)
install.packages("rpart")
install.packages('rpart.plot')
library('rpart.plot')
library(rpart)
#tree 1 for information gain 
# Using ID3 with rpart
dataset.id3 <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
printcp(dataset.id3)  # Display cp table for pruning
```

```{r}
# Plot the decision tree1
rpart.plot(dataset.id3)
```

```{r}
text(dataset.id3)
```

```{r}
# Make predictions on the test set
testPred <- predict(dataset.id3, newdata = testData, type = "class")
result <- table(testPred, testData$Status)
```

```{r}
#Evaluate the model:
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
co_result <- confusionMatrix(result)
print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))
acc <- co_result$overall["Accuracy"]
acc
```


The formula myFormula is used to define the target variable (Status) and
predictor variables for the classification. We made decision tree
classification using the ID3 algorithm (information gain) on a dataset
then we split the dataset into 70% training and 30% testing. For
visualization the tree In this case, the output lists the variables that
are actually used in constructing the classification tree which are the
following variables are used: Age, Estrogen.Status,
Reginol.Node.Positive, Regional.Node.Examined, Survival.Months to know the status (0 dead, 1 alive). Survival months has the
highest Information Gain so it's chosen as the splitting criterion at
each node of the decision tree.

For the Predictions , they are made on the test set using the trained
decision tree. we calculated confusion matrix, sensitivity, specificity,
precision, and overall accuracy . And the accuracy was high(88.4%) which
indicates that a significant portion of the predictions made by the
model match the actual outcomes in the test dataset as well as the other
measurments.

#Tree 2 
70% training and 30% and gini index
```{r}
# Using CART with rpart
#tree 2 using gini index
dataset.cart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))
# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#the result
testPredc <- predict(dataset.cart, newdata = testData, type = "class")
resultc <- table(testPredc, testData$Status)
```

```{r}
#Evaluate the model:
co_resultc <- confusionMatrix(resultc)
print(co_resultc)
sensitivity(as.table(co_resultc))
specificity(as.table(co_resultc))
precision(as.table(co_resultc))
accc <- co_resultc$overall["Accuracy"]
accc
```
We made decision tree classification using the CARAT algorithm (gini
index) on a dataset then we split the dataset into 70% training and 30%
testing. For visualization the tree In this case, the tree is built
using the variables Age, estrogen.Status, Reginol.Node.Positive, and Survival.Months to know the status (0 dead,
1 alive). Survival months has minimum Gini index so it's the root. For
the Predictions , they are made on the test set using the trained
decision tree. we calculated confusion matrix, sensitivity, specificity,
precision, and overall accuracy . And the accuracy was high(88.8%) which
indicates that a significant portion of the predictions made by the
model match the actual outcomes in the test dataset.


#Tree 3
70% training and 30% and gain ratio
```{r}
#tree3 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
dataset.c45 <- C5.0(myFormula, data = trainData)
# Print the tree rules
summary(dataset.c45)
```

```{r}
# Make predictions on the test set
testPreds <- predict(dataset.c45, newdata = testData)
results <- table(testPreds, testData$Status)
c45_party <- as.party(dataset.c45)
```

```{r}
# Plot the decision tree
plot(c45_party)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_results <- confusionMatrix(results)
print(co_results)
sensitivity(as.table(co_results))
specificity(as.table(co_results))
precision(as.table(co_results))
accs <- co_results$overall["Accuracy"]
accs

```
We made decision tree classification using the C5.0 algorithm (gain
ratio) on a dataset then we split the dataset into 70% training and 30%
testing. For visualization the tree In this case, The most important
attribute is "Survival.Months" with 100% usage, followed by
"Marital.status," "x6TH.stage," and "Reginol.Node.Positive". The size of
the tree is indicated by the number of nodes. In this case, the tree has
a size of 10 nodes. The tree has 225 errors, representing a
misclassification rate of 8% For the Predictions , they are made on the
test set using the trained decision tree. we calculated confusion
matrix, sensitivity, specificity, precision, and overall accuracy . And
the accuracy was high(88.6%) which indicates that a significant portion of the
predictions made by the model match the actual outcomes in the test
dataset.

#Tree 4 
80% training and 20% and information gain
```{r}

# Split the dataset into 80% training and 20% testing subsets
indfo <- sample(1:nrow(dataset), size = floor(0.8 * nrow(dataset)), replace = FALSE)
train.Data <- dataset[indfo,]
test.Data <- dataset[-indfo,]
```

```{r}
# tree4 using information gain
# Using ID3 with rpart
datasetID3 <- rpart(myFormula, data = train.Data, method = "class", parms = list(split = "information"))
print(datasetID3)  # Display cp table for pruning

# Plot the decision tree
rpart.plot(datasetID3)

```

```{r}
# Make predictions on the test set
testPredec <- predict(datasetID3, newdata = test.Data, type = "class")
resultdd<- table(testPredec, test.Data$Status)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
coresult <- confusionMatrix(resultdd)
print(coresult)
sensitivity(as.table(coresult))
specificity(as.table(coresult))
precision(as.table(coresult))
accur<- coresult$overall["Accuracy"]
accur
```

The formula myFormula is used to define the target variable (Status) and
predictor variables for the classification.

We made decision tree classification using the ID3 algorithm
(information gain) on a dataset then we split the dataset into 80%
training and 20% testing. For visualization the tree In this case, the
output lists the variables that are actually used in constructing the
classification tree which are the following variables are used: Age,
Estrogen.Status, X6TH.Stage, Survival.Months to know the status (0 dead,
1 alive). Survival months has the highest Information Gain so it's
chosen as the splitting criterion at each node of the decision tree.

For the Predictions , they are made on the test set using the trained
decision tree. we calculated confusion matrix, sensitivity, specificity,
precision, and overall accuracy . And the accuracy was high(90%) which
indicates that a significant portion of the predictions made by the
model match the actual outcomes in the test dataset.


#Tree 5
80% training and 20% and gini index
```{r}
#tree5 using gini index
# Using CART with rpart
datasetcart <- rpart(myFormula, data = train.Data, method = "class", parms = list(split = "gini"))

# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#result
testPredcs<- predict(datasetcart, newdata = test.Data, type = "class")
resultcs <- table(testPredcs, test.Data$Status)
```

```{r}
#Evaluate the model:
co_resultcg <- confusionMatrix(resultcs)
print(co_resultcg)
sensitivity(as.table(co_resultcg))
specificity(as.table(co_resultcg))
precision(as.table(co_resultcg))
acccu<- co_resultcg$overall["Accuracy"]
acccu
```
We made decision tree classification using the CARAT algorithm (gini
index) on a dataset then we split the dataset into 80% training and 20%
testing. For visualization the tree In this case, the tree is built
using the variables Age, Progesterone.Status, Reginol.Node.Positive,
, survival months to know the status (0 dead, 1 alive).
Survival months has minimum Gini index so it's the root.

For the Predictions , they are made on the test set using the trained
decision tree. we calculated confusion matrix, sensitivity, specificity,
precision, and overall accuracy . And the accuracy was high(90.5%) which
indicates that a significant portion of the predictions made by the
model match the actual outcomes in the test dataset as well as the other
measurements.

#Tree 6
80% training and 20% and gain ratio
```{r}
#tree6 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
datasetc45 <- C5.0(myFormula, data = train.Data)
# Print the tree rules
summary(datasetc45)
```

```{r}
# Make predictions on the test set
testPredsn <- predict(datasetc45, newdata = test.Data)
resultsn<- table(testPredsn, test.Data$Status)
c45party <- as.party(datasetc45)

```

```{r}
# Plot the decision tree
plot(c45party)

```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_resultsu <- confusionMatrix(resultsn)
print(co_resultsu)
sensitivity(as.table(co_resultsu))
specificity(as.table(co_resultsu))
precision(as.table(co_resultsu))
accsn <- co_resultsu$overall["Accuracy"]
accsn
```
```

The following is a 20% testing and 80% Training information gain ratio
created where the classification label is known (pre-classified) for
each record. Next, the algorithm systematically assigns each record to
one of two subsets on the some basis ( Survival.Month \> 0.429 or
Survival.Month \<=0.429). The object is to attain an homogeneous set of
labels in each partitionThis partitioning (splitting) is then applied to
each of the new partitions. The process continues until no more useful
splits can be found. The goal is to build a tree that distinguishes
among the classes. 
For visualization the tree In this case, The most important
attribute is "Survival.Months" with 100% usage, followed by
"estrogrn.status," "age," and "N.stage". The size of
the tree is indicated by the number of nodes. In this case, the tree has
a size of 10 nodes. The tree has 283 errors, representing a
misclassification rate of 8.8% For the Predictions , they are made on the
test set using the trained decision tree. we calculated confusion
matrix, sensitivity, specificity, precision, and overall accuracy . And
the accuracy was high(90%) which indicates that a significant portion of the
predictions made by the model match the actual outcomes in the test
dataset.

#Tree 7
60% training and 40% and infomation gain
```{r}
# Split the dataset into 60% training and 40% testing subsets
indfod <- sample(1:nrow(dataset), size = floor(0.6 * nrow(dataset)), replace = FALSE)
trainDataa <- dataset[indfod,]
testDataa <- dataset[-indfod,]
```

```{r}
#tree 7 using information gain
# Using ID3 with rpart
datasetID3.<- rpart(myFormula, data = trainDataa, method = "class", parms = list(split = "information"))
print(datasetID3)  # Display cp table for pruning

```

```{r}
# Plot the decision tree
rpart.plot(datasetID3.)
```

```{r}
# Make predictions on the test set
testPredecy<- predict(datasetID3, newdata = testDataa, type = "class")
resultddy<- table(testPredecy, testDataa$Status)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

coresulty <- confusionMatrix(resultddy)
print(coresulty)
sensitivity(as.table(coresulty))
specificity(as.table(coresulty))
precision(as.table(coresulty))
accury<- coresulty$overall["Accuracy"]
accury
```
The formula myFormula is used to define the target variable (Status) and
predictor variables for the classification.
We made decision tree classification using the ID3 algorithm
(information gain) on a dataset then we split the dataset into 80%
training and 20% testing. Next, the algorithm systematically
assigns each record to one of two subsets on the some basis whether (
Survival.Month \<0.43) or not . The object is to attain an homogeneous
set of labels in each partitionThis partitioning (splitting) is then
applied to each of the new partitions. The process continues until no
more useful splits can be found. The goal is to build a tree that
distinguishes among the classes. 
For visualization the tree In this case, the
output lists the variables that are actually used in constructing the
classification tree which are the following variables are used: Age, X6TH.Stage,tumor.size, differentiate, Survival.Months to know the status (0 dead,
1 alive). Survival months has the highest Information Gain so it's
chosen as the splitting criterion at each node of the decision tree.

For the Predictions , they are made on the test set using the trained
decision tree. we calculated confusion matrix, sensitivity, specificity,
precision, and overall accuracy . And the accuracy was high(91%) which
indicates that a significant portion of the predictions made by the
model match the actual outcomes in the test dataset.

#Tree 8
60% training and 40% and gini index
```{r}
#tree 8 using gini index
# Using CART with rpart
datasetcart. <- rpart(myFormula, data = trainDataa, method = "class", parms = list(split = "gini"))

# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#result

testPredcse<- predict(datasetcart., newdata = testDataa, type = "class")
resultcse <- table(testPredcse, testDataa$Status)
```

```{r}
#Evaluate the model:
co_resultcge <- confusionMatrix(resultcse)
print(co_resultcge)
sensitivity(as.table(co_resultcge))
specificity(as.table(co_resultcge))
precision(as.table(co_resultcge))
acccue<- co_resultcge$overall["Accuracy"]
acccue
```
The following is a 40% testing and 60% Training gini index created where the classification label is known (pre-classified) for each record.
Next, the algorithm systematically assigns each record to one of two
subsets on the some basis whether ( Survival.Month \<0.43) or not . The
object is to attain an homogeneous set of labels in each partition This
partitioning (splitting) is then applied to each of the new partitions.
The process continues until no more useful splits can be found. The goal
is to build a tree that distinguishes among the classes.
For visualization the tree In this case, the tree is built
using the variables Age, Progesterone.Status, Reginol.Node.Positive,
, survival months to know the status (0 dead, 1 alive).
Survival months has minimum Gini index so it's the root.

For the Predictions , they are made on the test set using the trained
decision tree. we calculated confusion matrix, sensitivity, specificity,
precision, and overall accuracy . And the accuracy was high(90.5%) which
indicates that a significant portion of the predictions made by the
model match the actual outcomes in the test dataset as well as the other
measurements.

```{r}
#tree 9 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
datasetc45. <- C5.0(myFormula, data = trainDataa)

# Print the tree rules
summary(datasetc45.)
```

```{r}

# Make predictions on the test set
testPredsni <- predict(datasetc45., newdata = testDataa)
resultsni<- table(testPredsni, testDataa$Status)
c45party. <- as.party(datasetc45.)
```

```{r}
# Plot the decision tree
plot(c45party.)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
co_resultsui <- confusionMatrix(resultsni)
print(co_resultsui)
sensitivity(as.table(co_resultsui))
specificity(as.table(co_resultsui))
precision(as.table(co_resultsui))
accsni <- co_resultsui$overall["Accuracy"]
accsni
```
The following is a 40% testing and 60% Training information gain ratio
created where the classification label is known (pre-classified) for
each record. Next, the algorithm systematically assigns each record to
one of two subsets on the some basis whether ( Survival.Month \> 0.429
or Survival.Month \<=0.429).not . The object is to attain an homogeneous
set of labels in each partitionThis partitioning (splitting) is then
applied to each of the new partitions. The process continues until no
more useful splits can be found. The goal is to build a tree that
distinguishes among the classes. 
For visualization the tree In this case, The most important
attribute is "Survival.Months" with 100% usage, followed by
"age"," "estrogen.status," and ,"Reginol.Node.Positive",T.stage". The size of
the tree is indicated by the number of nodes. In this case, the tree has
a size of 10 nodes. The tree has 218 errors, representing a
misclassification rate of 9.1% For the Predictions , they are made on the
test set using the trained decision tree. we calculated confusion
matrix, sensitivity, specificity, precision, and overall accuracy . And
the accuracy was high(90.5%) which indicates that a significant portion of the
predictions made by the model match the actual outcomes in the test
dataset.

In summary, the three partitions (0.7 , 0.3 and 0.6 , 0.4 and 0.8 , 0.2)
and the three attributes selection (IG, IG ratio, gini index) lead to
similar results , such as all of them have high accuracy above(88% to 91%) and
other measurements have similar results. This is a positive sign
indicating that the model's performance is consistent and not overly
dependent on specific choices. Information Gain, Information Gain Ratio,
and Gini Index algorithms lead to similar results, it indicates that,
these metrics are providing comparable insights into the dataset.
consistency across different partitions and attribute selection methods
is a positive signal, it's advisable to perform thorough evaluation,
including testing on a separate dataset, to ensure the model's
reliability and generalization capabilities.

# Clustring
# Clustering is unsupervised learning so there is no class label, it is the task of dividing the unlabeled data or data points into different clusters such that similar data points fall in the same cluster than those which differ from the others. In simple words, the aim of the clustering process is to segregate groups with similar traits and assign them into clusters.
# in this project we used the K-means algorithm, it is a centroid-based clustering algorithm. it works randomly selecting K objects as the centroids of K clusters. Then, it assigns the other objects to the nearest cluster to them by measuring the distance between the centroid and each object. Consequently, updates the clusters and compute the new centroids, then repeats until no significant change of clusters occurs. For the approach to work, numerical data and prior knowledge of the number of clusters K are required, K-Means works well with our dataset because we converted every entry to a numeric value in the stage of preprocessing.
# K-Means clustering was applied with the assistance of the the R packages and packages listed below:
# factoextra package: Extract and Visualize the Results of Multivariate Data Analyses.
# cluster package: provides methods for Cluster analysis.
# NbClust package: provides 30 indexes for determining the optimal number of clusters in data set and offers the best clustering scheme from different results.
# set.seed(): Create reproducible results
# kmeans(): Clusters data based on similarity.
# fviz_cluster(): Visualize clustering results.
# silhouette(): Find the average for each and all clusters.
# fviz_silhouette(): Visualize the silhouette information.
# fviz_nbclust(): Determining the optimal number of clusters.


# We will experiment with the following three values of k: (3,4,5). We will compute the average silhouette ,total within-cluster sum of square and the BCubed metrics (precision and recall) for each K.


# Clustr of size=3
# this number was chosen after applying the silhouette method which shows that 3 is the optimal number for clustring
# This code applies the k-means clustering algorithm on the dataset, and then evaluates the clustering results using various metrics, including silhouette analysis and BCubed metrics.The BCubed metrics provide an evaluation of the clustering results by comparing them to ground truth label, assuming they are available in the 'datawithcl' dataset.
```{r}

library(factoextra)
library(NbClust)
library(cluster)

datawithcl<-dataset #dataset with the class label) # dataset with a class label
dataset <- dataset[, -which(names(dataset) == "Status")] #creates a copy of the dataset called datawithcl with the class label and then removes the class label column from the original dataset

str(dataset)

#converting intger columns in the dataset to numeric format. The columns converted include 'Race,' 'Marital.Status,' 'T.Stage,' 'N.Stage,' 'X6th.Stage,' 'differentiate,' 'Grade,' 'A.Stage,' 'Estrogen.Status,' and 'Progesterone.Status' since we are applying k-means this is imoprtant step
numeric_dataset = dataset
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
str(numeric_dataset)

#appling the k-means clustering algorithm (kmeans function) to the preprocessed dataset with three clusters (centers = 3). The maximum number of iterations is set to 140, and the algorithm used is "Lloyd"which is an iterative method used to solve the k-means clustering problem. The algorithm minimizes the sum of squared distances between data points and the centroids (mean points) of their respective clusters. and The results are stored in the variable 'kmns'

kmns <- kmeans(dataset, 3, iter.max = 140 , algorithm="Lloyd", nstart=100) 
kmns

#using the fviz_cluster function from the factoextra library to visualize the clustering results. The visualization includes points representing data, cluster centers, and ellipses around clusters.

library(factoextra)
fviz_cluster(kmns, data = dataset, geom = "point", 
             show.clust.cent = FALSE, ellipse = TRUE, 
             repel = TRUE, ggtheme = theme_minimal())



#calculates silhouette scores for the clustering results using the silhouette function and visualizes them using fviz_silhouette.
#Cluster Validation
library(cluster)
#average for each cluster
avg_sil <- silhouette(kmns$cluster,dist(dataset))
fviz_silhouette(avg_sil)


#calculates and prints the within-cluster sum of squares (WCSS), which is a measure of the compactness of clusters. and print it
# Calculate the WCSS
wcss <- sum(kmns$withinss)

# Print the WCSS
print(wcss)


#defines a function 'calculate_bcubed_metrics' to compute BCubed precision and recall. The function takes cluster assignments and ground truth labels(Status) as input and returns precision and recall. Then, it calculates and prints BCubed precision and recall for the clustering results.
#BCubed Metrics
cluster_assignments <- kmns$cluster
ground_truth_labels <- datawithcl$Status

calculate_bcubed_metrics <- function(cluster_assignments, ground_truth_labels) {
  precision_sum <- recall_sum <- 0
  
  for (i in seq_along(cluster_assignments)) {
    same_category_same_cluster <- sum(ground_truth_labels[cluster_assignments == cluster_assignments[i]] == ground_truth_labels[i])
    total_same_cluster <- sum(cluster_assignments == cluster_assignments[i])
    total_same_category <- sum(ground_truth_labels == ground_truth_labels[i])
    
    precision_sum <- precision_sum + same_category_same_cluster / total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  precision <- precision_sum / length(cluster_assignments)
  recall <- recall_sum / length(cluster_assignments)
  
  return(list(precision = precision, recall = recall))
}

metrics <- calculate_bcubed_metrics(cluster_assignments, ground_truth_labels)
precision <- metrics$precision
recall <- metrics$recall

cat("BCubed Precision:", precision)
cat("BCubed Recall:", recall)

```

#Clustr of size=4
#this number was chosen randomlly
```{r}

library(factoextra)
library(NbClust)
library(cluster)

datawithcl<-dataset #dataset with the class label) # dataset with a class label
dataset <- dataset[, -which(names(dataset) == "Status")] #creates a copy of the dataset called datawithcl with the class label and then removes the class label column from the original dataset
str(dataset)
```
```{r}
#converting intger columns in the dataset to numeric format. The columns converted include 'Race,' 'Marital.Status,' 'T.Stage,' 'N.Stage,' 'X6th.Stage,' 'differentiate,' 'Grade,' 'A.Stage,' 'Estrogen.Status,' and 'Progesterone.Status' since we are applying k-means this is imoprtant step
numeric_dataset = dataset
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
str(numeric_dataset)

#appling the k-means clustering algorithm (kmeans function) to the preprocessed dataset with three clusters (centers = 4). The maximum number of iterations is set to 140, and the algorithm used is "Lloyd"which is an iterative method used to solve the k-means clustering problem. The algorithm minimizes the sum of squared distances between data points and the centroids (mean points) of their respective clusters. and The results are stored in the variable 'kmns'

kmns <- kmeans(dataset, 4, iter.max = 140 , algorithm="Lloyd", nstart=100) 
kmns
```
```{r}
#using the fviz_cluster function from the factoextra library to visualize the clustering results. The visualization includes points representing data, cluster centers, and ellipses around clusters.

library(factoextra)
fviz_cluster(kmns, data = dataset, geom = "point", 
             show.clust.cent = FALSE, ellipse = TRUE, 
             repel = TRUE, ggtheme = theme_minimal())
```

```{r}
#calculates silhouette scores for the clustering results using the silhouette function and visualizes them using fviz_silhouette.
#Cluster Validation
library(cluster)
#average for each cluster
avg_sil <- silhouette(kmns$cluster,dist(dataset))
fviz_silhouette(avg_sil)


#calculates and prints the within-cluster sum of squares (WCSS), which is a measure of the compactness of clusters. and print it
# Calculate the WCSS
wcss <- sum(kmns$withinss)

# Print the WCSS
print(wcss)


#defines a function 'calculate_bcubed_metrics' to compute BCubed precision and recall. The function takes cluster assignments and ground truth labels(Status) as input and returns precision and recall. Then, it calculates and prints BCubed precision and recall for the clustering results.
#BCubed Metrics
cluster_assignments <- kmns$cluster
ground_truth_labels <- datawithcl$Status

calculate_bcubed_metrics <- function(cluster_assignments, ground_truth_labels) {
  precision_sum <- recall_sum <- 0
  
  for (i in seq_along(cluster_assignments)) {
    same_category_same_cluster <- sum(ground_truth_labels[cluster_assignments == cluster_assignments[i]] == ground_truth_labels[i])
    total_same_cluster <- sum(cluster_assignments == cluster_assignments[i])
    total_same_category <- sum(ground_truth_labels == ground_truth_labels[i])
    
    precision_sum <- precision_sum + same_category_same_cluster / total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  precision <- precision_sum / length(cluster_assignments)
  recall <- recall_sum / length(cluster_assignments)
  
  return(list(precision = precision, recall = recall))
}

metrics <- calculate_bcubed_metrics(cluster_assignments, ground_truth_labels)
precision <- metrics$precision
recall <- metrics$recall

cat("BCubed Precision:", precision)
cat("BCubed Recall:", recall)
```




#Clustr of size=5
#this number was chosen randomlly
```{r}

library(factoextra)
library(NbClust)
library(cluster)

datawithcl<-dataset #dataset with the class label) # dataset with a class label
dataset <- dataset[, -which(names(dataset) == "Status")] #creates a copy of the dataset called datawithcl with the class label and then removes the class label column from the original dataset

str(dataset)

#converting intger columns in the dataset to numeric format. The columns converted include 'Race,' 'Marital.Status,' 'T.Stage,' 'N.Stage,' 'X6th.Stage,' 'differentiate,' 'Grade,' 'A.Stage,' 'Estrogen.Status,' and 'Progesterone.Status' since we are applying k-means this is imoprtant step
numeric_dataset = dataset
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
str(numeric_dataset)

#appling the k-means clustering algorithm (kmeans function) to the preprocessed dataset with three clusters (centers = 5). The maximum number of iterations is set to 140, and the algorithm used is "Lloyd"which is an iterative method used to solve the k-means clustering problem. The algorithm minimizes the sum of squared distances between data points and the centroids (mean points) of their respective clusters. and The results are stored in the variable 'kmns'

kmns <- kmeans(dataset, 5, iter.max = 140 , algorithm="Lloyd", nstart=100) 
kmns

#using the fviz_cluster function from the factoextra library to visualize the clustering results. The visualization includes points representing data, cluster centers, and ellipses around clusters.

library(factoextra)
fviz_cluster(kmns, data = dataset, geom = "point", 
             show.clust.cent = FALSE, ellipse = TRUE, 
             repel = TRUE, ggtheme = theme_minimal())



#calculates silhouette scores for the clustering results using the silhouette function and visualizes them using fviz_silhouette.
#Cluster Validation
library(cluster)
#average for each cluster
avg_sil <- silhouette(kmns$cluster,dist(dataset))
fviz_silhouette(avg_sil)


#calculates and prints the within-cluster sum of squares (WCSS), which is a measure of the compactness of clusters. and print it
# Calculate the WCSS
wcss <- sum(kmns$withinss)

# Print the WCSS
print(wcss)


#defines a function 'calculate_bcubed_metrics' to compute BCubed precision and recall. The function takes cluster assignments and ground truth labels(Status) as input and returns precision and recall. Then, it calculates and prints BCubed precision and recall for the clustering results.
#BCubed Metrics
cluster_assignments <- kmns$cluster
ground_truth_labels <- datawithcl$Status

calculate_bcubed_metrics <- function(cluster_assignments, ground_truth_labels) {
  precision_sum <- recall_sum <- 0
  
  for (i in seq_along(cluster_assignments)) {
    same_category_same_cluster <- sum(ground_truth_labels[cluster_assignments == cluster_assignments[i]] == ground_truth_labels[i])
    total_same_cluster <- sum(cluster_assignments == cluster_assignments[i])
    total_same_category <- sum(ground_truth_labels == ground_truth_labels[i])
    
    precision_sum <- precision_sum + same_category_same_cluster / total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  precision <- precision_sum / length(cluster_assignments)
  recall <- recall_sum / length(cluster_assignments)
  
  return(list(precision = precision, recall = recall))
}

metrics <- calculate_bcubed_metrics(cluster_assignments, ground_truth_labels)
precision <- metrics$precision
recall <- metrics$recall

cat("BCubed Precision:", precision)
cat("BCubed Recall:", recall)
```





# Since choosing the optimal number of clusters can help represinting the data in the best way possible and saves much time. we employed the silhouette method to guide the selection of the optimal number of clusters
# we have used "fviz_nbclust()" with silhouette method. it indicates that the optimal number of clusters is K=3.
```{r}
# 2- silhouette method
#a)fviz_nbclust() with silhouette method using library(factoextra)
fviz_nbclust(dataset, kmeans, method = "silhouette")+labs(subtitle ="Silhouette method")
```
# The plot consist of a line or curve that represents the silhouette scores for different numbers of clusters (k), The number of clusters in this graph is fixed at three. The ideal number of clusters is shown by the graph's highest point. This indicates the number of clusters that provide the optimal clustering solution and number of clusters that  is often associated with a peak in the silhouette scores



```{r}

```

```{r}


```

```{r}



```



# Evaluation and Comparison
+--------------------+-----------------------------------------------------------------------------------------------------------------------------+
| Mining task :     | Comparison Criteria :                                                                                                       |
+====================+=============================================================================================================================+
| **Classification** | +-----------------------+------------------------------+------------------------------+-----------------------------+       |
|                    | |                       | 60% training set             | 70% training set             | 80% training set            |       |
|                    | |                       |                              |                              |                             |       |
|                    | |                       | 40% testing set              | 30% testing set              | 20% testing set             |       |
|                    | +=======================+==============================+==============================+=============================+       |
|                    | | **Accuracy**          |                              |                              |                             |       |
|                    | +-----------------------+------------------------------+------------------------------+-----------------------------+       |
|                    | | **Precision**         |                              |                              |                             |       |
|                    | +-----------------------+------------------------------+------------------------------+-----------------------------+       |
|                    | | **Sensitivity**       |                              |                              |                             |       |
|                    | +-----------------------+------------------------------+------------------------------+-----------------------------+       |
|                    | | **Specificity**       |                              |                              |                             |       |
|                    | +-----------------------+------------------------------+------------------------------+-----------------------------+       |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------+
| **Clustering**     | +----------------------------------------+--------------------------+-------------------------+---------------------------+ |
|                    | |                                        | K=4                      | K=5                     | K=6                       | |
|                    | +========================================+==========================+=========================+===========================+ |
|                    | | **Average silhouette width**           |                          |                         |                           | |
|                    | +----------------------------------------+--------------------------+-------------------------+---------------------------+ |
|                    | | **total within-cluster sum of square** |                          |                         |                           | |
|                    | +----------------------------------------+--------------------------+-------------------------+---------------------------+ |
|                    | | **BCubed precision**                   |                          |                         |                           | |
|                    | +----------------------------------------+--------------------------+-------------------------+---------------------------+ |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------+


# Findings






# References 
