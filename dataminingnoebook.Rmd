---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


#importing dataset
```{r}
setwd("C:/Users/saran/OneDrive")
dataset = read.csv('BreastCancer1.csv')
str(dataset)
library(Hmisc)
describe(dataset)
summary(dataset)
```
#histogram for ages
```{r}
hist(dataset$Age)
```
#A histogram showing the age distribution of women diagnosed with breast cancer. The histogram shows that breast cancer is more common among women whose ages range from 45 t0 50 years(highest frequency).

#pie chart for race
```{r}
library(dplyr)
tab = dataset$Race %>% table()
precentages = tab %>% prop.table() %>% round(3)*100
txt = paste0(names(tab), '\n', precentages, '%')
pie(tab, labels = txt, main= 'Race')
```
#The pie chat divided the races into three sections with their percentage ( as our dataset shows we have white, black , other). As we see, the white race preset the highest percentage ( 84.8%).




#Scatter Plot
```{r}
with(dataset, plot(Tumor.Size, Survival.Months, col = "blue", pch = 16))
#This scatter plot has no correlation although we can see that when the tumor size is small the human is more likely to live more months which make sense
```




#changing the value IV in Grade attribute
```{r}
dataset$Grade[dataset$Grade == ' anaplastic; Grade IV' ] = 4 
table(dataset$Grade)
```
#install.packages("outliers")
```{r}
library(outliers)
#outliers of age
outage= outlier(dataset$Age)
print(outage)
#number of rows that have the outlier 
table(dataset$Age == outage )

#outliers of tumor size
outTumorSize = outlier(dataset$Tumor.Size)
print(outTumorSize)
#number of rows that has the outlier
table(dataset$Tumor.Size == outTumorSize)

#outliers of Regional Node Examined
outNodeExamined = outlier(dataset$Regional.Node.Examined)
print(outNodeExamined)
#number of rows that has the outlier
table(dataset$Regional.Node.Examined == outNodeExamined )

#outliers of Reginol Node Positive
outNodepositive = outlier(dataset$Reginol.Node.Positive)
print(outNodepositive)
#number of rows that has the outlier
table(dataset$Reginol.Node.Positive == outNodepositive )

#outliers of Survival Months
outmonth = outlier(dataset$Survival.Months)
print(outmonth)
#number of rows that has the outlier 
table(dataset$Survival.Months==outmonth)
```

```{r}
#delete the outliers
dataset = dataset[dataset$Age != outage ,]
dataset = dataset[dataset$Tumor.Size != outTumorSize ,]
dataset = dataset[dataset$Regional.Node.Examined != outNodeExamined ,]
dataset = dataset[dataset$Reginol.Node.Positive != outNodepositive ,]
dataset = dataset[dataset$Survival.Months != outmonth ,]
nrow(dataset)
```
#checking if there is any missing value or duplicated rows 
is.na(dataset)
sum(is.na(dataset))
sum(duplicated(dataset))
#removing duplicate
dataset= unique(dataset)
sum(duplicated(dataset))
#number of rows after deleting dup
nrow(dataset)
nrow(dataset)

#Normalization:
#Define function normalize
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}

#Call normalize function
dataset$Age<-normalize(dataset$Age)
dataset$Tumor.Size<-normalize(dataset$Tumor.Size)
dataset$Regional.Node.Examined<-normalize(dataset$Regional.Node.Examined)
dataset$Reginol.Node.Positive<-normalize(dataset$Reginol.Node.Positive)
dataset$Survival.Months<-normalize(dataset$Survival.Months)


#showing normalized columns
print(dataset)

#encoding
dataset$Race = factor(dataset$Race,levels = c("White","Black", "Other"), labels=c(1,2,3))
dataset$Marital.Status = factor(dataset$Marital.Status,levels = c("Married","Divorced","Separated","Single ","Widowed"), labels=c(1,2,3,4,5))
dataset$differentiate = factor(dataset$differentiate,levels = c("Well differentiated","Moderately differentiated", "Poorly differentiated","Undifferentiated"), labels=c(1,2,3,4))
dataset$X6th.Stage = factor(dataset$X6th.Stage,levels = c("IIA","IIIA", "IIB","IIIB","IIIC"), labels=c(1,2,3,4,5))
dataset$Status = factor(dataset$Status,levels = c("Dead","Alive"), labels=c(0,1))
dataset$Estrogen.Status = factor(dataset$Estrogen.Status ,levels = c("Negative","Positive"), labels=c(0,1))
dataset$Progesterone.Status = factor(dataset$Progesterone.Status ,levels = c("Negative","Positive"), labels=c(0,1))
dataset$A.Stage	 = factor(dataset$A.Stage	,levels = c("Regional","Distant"), labels=c(0,1))
dataset$T.Stage	 = factor(dataset$T.Stage	,levels = c("T1","T2","T3","T4"), labels=c(1,2,3,4))
dataset$N.Stage	 = factor(dataset$N.Stage	,levels = c("N1","N2","N3"), labels=c(1,2,3))

is.na(dataset)
sum(is.na(dataset))

#feature selection
# Get the column names of the dataset
column_names <- colnames(dataset)

# Perform chi-square test for each attribute
for (attribute in column_names) {
  if (attribute != "Status") {
    # Create a contingency table
    contingency_table <- table(dataset[[attribute]], dataset$Status)
    
    # Perform chi-square test
    result <- chisq.test(contingency_table)
    
    # Print the attribute name and the result
    cat("Attribute:", attribute, "\n")
    print(result)
    cat("\n")
  }
}
#the chi-square test results indicate that most of the attributes have a significant association with the variable being tested. These attributes include Age, Race, Marital.Status, T.Stage, N.Stage, X6th.Stage, differentiate, Grade, A.Stage, Estrogen.Status, Progesterone.Status, Regional.Node.Examined, Reginol.Node.Positive, Tumor.Size, Survival.Months, and Status. The low p-values provide strong evidence to reject the null hypothesis of no association between these attributes and the variable.
#Since they are highly correlated there is no need to perform feature selection






#other way to choose the relevant attributes to the dataset, we performed this feature selection if you want to check it out, although we think the chi square is better
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# Split the dataset into features and target variable
features <- dataset[, -c(2 , 3 ,16)]  # Exclude non-numeric and target variable columns
target <- dataset$Status

# Define the control parameters for RFE
ctrl <- rfeControl(functions = rfFuncs, 
                   method = "cv",
                   number = 10)  # 10-fold cross-validation

# Assuming 'rfFuncs' is correctly defined for Recursive Feature Elimination using Random Forest
rfe_result <- rfe(features, target, sizes = 1:ncol(features), rfeControl = ctrl)

# Print the RFE result
print(rfe_result)
plot(rfe_result)
#This feature selection graphical representation shows the higher-ranked features that considered more important or relevant for predicting the target variable which are: Survival.Months, Reginol.Node.Positive, X6th.Stage, Progesterone. Status, Age.,this helps choosing which features to include or exclude . so we can improve model efficiency, reduce overfitting, and enhance interpretability by focusing on the most relevant features for the task at hand.

#Classification
```{r}
dataset$Status <- factor(dataset$Status)
# Set the random seed for reproducibility
set.seed(1234)

# Split the dataset into training 70% and testing 30% subsets
ind <- sample(2, nrow(dataset), replace = TRUE, prob = c(0.7 , 0.3))
trainData <- dataset[ind == 1,]
testData <- dataset[ind == 2,]
myFormula <- Status ~ Age + Marital.Status + Race + T.Stage + N.Stage + X6th.Stage + differentiate + Grade + A.Stage + Tumor.Size + Estrogen.Status + Progesterone.Status + Reginol.Node.Positive + Regional.Node.Examined + Survival.Months 
```

```{r}
# Load the necessary library
install.packages('party')
library(party)
install.packages("rpart")
install.packages('rpart.plot')
library('rpart.plot')
library(rpart)
#tree 1 for information gain 
# Using ID3 with rpart
dataset.id3 <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
printcp(dataset.id3)  # Display cp table for pruning
```

```{r}
# Plot the decision tree1
rpart.plot(dataset.id3)
```

```{r}
text(dataset.id3)
```

```{r}
# Make predictions on the test set
testPred <- predict(dataset.id3, newdata = testData, type = "class")
result <- table(testPred, testData$Status)
```

```{r}
#Evaluate the model:
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
co_result <- confusionMatrix(result)
print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))
acc <- co_result$overall["Accuracy"]
acc
```

```{r}
# Using CART with rpart
#tree 2 using gini index
dataset.cart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))
# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#the result
testPredc <- predict(dataset.cart, newdata = testData, type = "class")
resultc <- table(testPredc, testData$Status)
```

```{r}
#Evaluate the model:
co_resultc <- confusionMatrix(resultc)
print(co_resultc)
sensitivity(as.table(co_resultc))
specificity(as.table(co_resultc))
precision(as.table(co_resultc))
accc <- co_resultc$overall["Accuracy"]
accc
```

```{r}
#tree3 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
dataset.c45 <- C5.0(myFormula, data = trainData)
# Print the tree rules
summary(dataset.c45)
```

```{r}
# Make predictions on the test set
testPreds <- predict(dataset.c45, newdata = testData)
results <- table(testPreds, testData$Status)
c45_party <- as.party(dataset.c45)
```

```{r}
# Plot the decision tree
plot(c45_party)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_results <- confusionMatrix(results)
print(co_results)
sensitivity(as.table(co_results))
specificity(as.table(co_results))
precision(as.table(co_results))
accs <- co_results$overall["Accuracy"]
accs

```

```{r}
# Split the dataset into 80% training and 20% testing subsets
indfo <- sample(2, nrow(dataset), replace = TRUE, prob = c(0.8 , 0.2))
trainData <- dataset[indfo == 1,]
testData <- dataset[indfo == 2,]
```

```{r}
# tree4 using information gain
# Using ID3 with rpart
datasetID3 <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
print(datasetID3)  # Display cp table for pruning

# Plot the decision tree
rpart.plot(datasetID3)
text(datasetID3)

```

```{r}
# Make predictions on the test set
testPredec <- predict(datasetID3, newdata = testData, type = "class")
resultdd<- table(testPredec, testData$Status)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
coresult <- confusionMatrix(resultdd)
print(coresult)
sensitivity(as.table(coresult))
specificity(as.table(coresult))
precision(as.table(coresult))
accur<- coresult$overall["Accuracy"]
accur
```

```{r}
#tree5 using gini index
# Using CART with rpart
datasetcart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))

# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#result
testPredcs<- predict(datasetcart, newdata = testData, type = "class")
resultcs <- table(testPredcs, testData$Status)
```

```{r}
#Evaluate the model:
co_resultcg <- confusionMatrix(resultcs)
print(co_resultcg)
sensitivity(as.table(co_resultcg))
specificity(as.table(co_resultcg))
precision(as.table(co_resultcg))
acccu<- co_resultcg$overall["Accuracy"]
acccu
```

```{r}
#tree6 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
datasetc45 <- C5.0(myFormula, data = trainData)
# Print the tree rules
summary(datasetc45)
```

```{r}
# Make predictions on the test set
testPredsn <- predict(datasetc45, newdata = testData)
resultsn<- table(testPredsn, testData$Status)
c45party <- as.party(datasetc45)

```

```{r}
# Plot the decision tree
plot(c45party)

```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_resultsu <- confusionMatrix(resultsn)
print(co_resultsu)
sensitivity(as.table(co_resultsu))
specificity(as.table(co_resultsu))

accsn <- co_resultsu$overall["Accuracy"]
accsn
```

```{r}
# Split the dataset into 60% training and 40% testing subsets
indfod<- sample(2, nrow(dataset), replace = TRUE, prob = c(0.6 , 0.4))
trainData <- dataset[indfod== 1,]
testData <- dataset[indfod == 2,]
```

```{r}
#tree 7 using information gain
# Using ID3 with rpart
datasetID3.<- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
print(datasetID3)  # Display cp table for pruning

```

```{r}
# Plot the decision tree
rpart.plot(datasetID3.)
text(datasetID3.)
```

```{r}
# Make predictions on the test set
testPredecy<- predict(datasetID3, newdata = testData, type = "class")
resultddy<- table(testPredecy, testData$Status)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

coresulty <- confusionMatrix(resultddy)
print(coresulty)
sensitivity(as.table(coresulty))
specificity(as.table(coresulty))
precision(as.table(coresulty))
accury<- coresulty$overall["Accuracy"]
accury
```

```{r}
#tree 8 using gini index
# Using CART with rpart
datasetcart. <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))

# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#result

testPredcse<- predict(datasetcart., newdata = testData, type = "class")
resultcse <- table(testPredcse, testData$Status)
```

```{r}
#Evaluate the model:
co_resultcge <- confusionMatrix(resultcse)
print(co_resultcge)
sensitivity(as.table(co_resultcge))
specificity(as.table(co_resultcge))
precision(as.table(co_resultcge))
acccue<- co_resultcge$overall["Accuracy"]
acccue
```

```{r}
#tree 9 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
datasetc45. <- C5.0(myFormula, data = trainData)

# Print the tree rules
summary(datasetc45.)
```

```{r}

# Make predictions on the test set
testPredsni <- predict(datasetc45., newdata = testData)
resultsni<- table(testPredsni, testData$Status)
c45party. <- as.party(datasetc45.)
```

```{r}
# Plot the decision tree
plot(c45party.)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
co_resultsui <- confusionMatrix(resultsni)
print(co_resultsui)
sensitivity(as.table(co_resultsui))
specificity(as.table(co_resultsui))
precision(as.table(co_resultsui))
accsni <- co_resultsui$overall["Accuracy"]
accsni
```
#Clustring
#For clustering I chose k-medioid method because it the best fit with my dataset, I tried three Ks 4,5,6.
By my understanding I found that 4 is the best K because the points well
distributed. I chose the attributes survival months and age after trying multiple attributes i found they give the best results.

The resulting plot displays a scatter plot where each data point is represented as a point on the plot. The points are colored based on their assigned cluster, allowing for visual exploration of the clustering results. The plot title and axis labels provide additional context for interpreting the plot, this plot shows 4 clusters for this dataset and is chosen as the optimal k number

#Clustr of size=4
#this number was chosen after applying the elbow method which shows that 5 is the optimal number for clustring
```{r}
#Converting interger&factor columns to numeric SO we can perform k-means
numeric_dataset = dataset
numeric_dataset$Age = as.numeric(as.character(dataset$Age))
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Tumor.Size = as.numeric(as.character(dataset$Tumor.Size))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
numeric_dataset$Regional.Node.Examined = as.numeric(as.character(dataset$Regional.Node.Examined))
numeric_dataset$Reginol.Node.Positive = as.numeric(as.character(dataset$Reginol.Node.Positive))
numeric_dataset$Survival.Months = as.numeric(as.character(dataset$Survival.Months))
numeric_dataset$Status = as.numeric(as.character(dataset$Status))
str(numeric_dataset)
library(cluster)
library(ggplot2)
# Assuming 'data' is your object
if (class(data) == "data.frame") {
  # Your code for subsetting columns here
} else {
  # Handle the case where 'data' is not a data frame
  print("The 'data' object is not a data frame.")
}

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}
# Example usage
selected_columns <- c('Age', "Survival.Months")  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 4 # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_assignments)


# Create scatter plot with colored clusters
ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")
```

#Clustr of size=5
#this number was chosen randomlly
```{r}
#Converting interger&factor columns to numeric SO we can perform k-means
numeric_dataset = dataset
numeric_dataset$Age = as.numeric(as.character(dataset$Age))
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Tumor.Size = as.numeric(as.character(dataset$Tumor.Size))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
numeric_dataset$Regional.Node.Examined = as.numeric(as.character(dataset$Regional.Node.Examined))
numeric_dataset$Reginol.Node.Positive = as.numeric(as.character(dataset$Reginol.Node.Positive))
numeric_dataset$Survival.Months = as.numeric(as.character(dataset$Survival.Months))
numeric_dataset$Status = as.numeric(as.character(dataset$Status))
str(numeric_dataset)
library(cluster)
library(ggplot2)
# Assuming 'data' is your object
if (class(data) == "data.frame") {
  # Your code for subsetting columns here
} else {
  # Handle the case where 'data' is not a data frame
  print("The 'data' object is not a data frame.")
}

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}
# Example usage
selected_columns <- c('Age', "Survival.Months")  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 5 # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_assignments)


# Create scatter plot with colored clusters
ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")
```




#Clustr of size=6
#this number was chosen after applying Silhouette coefficient which indicate 6 as the optimal k
```{r}
#Converting interger&factor columns to numeric SO we can perform k-means
numeric_dataset = dataset
numeric_dataset$Age = as.numeric(as.character(dataset$Age))
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Tumor.Size = as.numeric(as.character(dataset$Tumor.Size))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
numeric_dataset$Regional.Node.Examined = as.numeric(as.character(dataset$Regional.Node.Examined))
numeric_dataset$Reginol.Node.Positive = as.numeric(as.character(dataset$Reginol.Node.Positive))
numeric_dataset$Survival.Months = as.numeric(as.character(dataset$Survival.Months))
numeric_dataset$Status = as.numeric(as.character(dataset$Status))
str(numeric_dataset)


library(cluster)
library(ggplot2)
# Assuming 'data' is your object
if (class(data) == "data.frame") {
  # Your code for subsetting columns here
} else {
  # Handle the case where 'data' is not a data frame
  print("The 'data' object is not a data frame.")
}

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}
# Example usage
selected_columns <- c('Age', "Survival.Months")  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 6 # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_assignments)


# Create scatter plot with colored clusters
ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")
```
#The elbow method is a technique used to determine the optimal number of clusters in a clustering analysis by looking for the "elbow" point in the plot, which is the point of diminishing returns in reducing the within-cluster sum of squares (WSS) as the number of clusters increases. The plot helps visualize this relationship and assists in determining an appropriate number of clusters for the given data. which is 4 in our dataset
#also choosing the optimal number of clusters can help represinting the data in the best way possible and saves much time

```{r}
library(cluster)
library(ggplot2)

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
max_k <- 10  # Maximum number of clusters to consider

wss_values <- vector("numeric", max_k)  # Empty vector to store WSS values

for (k in 1:max_k) {
  cluster_assignments <- k_medoids_clustering(selected_data, k)  # Perform k-medoids clustering
  wss <- 0  # Initialize within-cluster sum of squares (WSS) value
  
  # Calculate WSS for the current k
  for (i in 1:k) {
    cluster_points <- selected_data[cluster_assignments == i, , drop = FALSE]
    
    # Calculate medoid as the observation with minimum average dissimilarity
    cluster_distances <- proxy::dist(cluster_points)
    avg_dissimilarity <- rowMeans(as.matrix(cluster_distances))
    medoid_index <- which.min(avg_dissimilarity)
    medoid <- cluster_points[medoid_index, ]
    
    cluster_distances <- proxy::dist(rbind(cluster_points, medoid))
    wss <- wss + sum(cluster_distances[1:nrow(cluster_points)])
  }
  
  wss_values[k] <- wss  # Store WSS value
}

# Plot WSS values against k values
elbow_plot <- ggplot() +
  geom_line(aes(x = 1:max_k, y = wss_values)) +
  geom_point(aes(x = 1:max_k, y = wss_values)) +
  labs(title = "Elbow Method", x = "Number of Clusters (k)", y = "Within-Cluster Sum of Squares (WSS)")

print(elbow_plot)
```
#another code for elbow method
```{r}
# 3- Elbow method
#fviz_nbclust() with within cluster sums of squares (wss) method
library(factoextra) 
fviz_nbclust(dataset, pam, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```
The resulting plot helps visualize the trend in silhouette scores as the number of clusters (k) increases. It allows  to identify the optimal number of clusters based on the highest silhouette score, which indicates better clustering quality and separation of data points within their assigned clusters. here we have the optimal k=6
```{r}
# Perform scaling on the data
scaled_data <- scale(data)

# Required libraries
library(fpc)
library(ggplot2)

# Determine the optimal number of clusters using the silhouette method
sil_scores <- c()
for (k in 2:10) {
  kmedoids_result <- pam(scaled_data, k = k)
  dissimilarity <- as.dist(dist(scaled_data))
  sil_scores[k-1] <- cluster.stats(dissimilarity, kmedoids_result$clustering)$avg.silwidth
}

# Plot the silhouette scores
k_values <- 2:10
silhouette_df <- data.frame(k = k_values, silhouette_score = sil_scores)
ggplot(silhouette_df, aes(x = k, y = silhouette_score)) +
  geom_line() +
  geom_point() +
  labs(x = "Number of Clusters (k)", y = "Silhouette Score") +
  ggtitle("Silhouette Score vs. Number of Clusters")

```
#here you can run the cluster and see BCubed precision and recall result thet we performed on the clustering method with k=4, B-Cubed precision and recall values are both 1, it suggests that the cluster assignments perfectly match the true labels. This indicates that the clustering algorithm has achieved a perfect separation of the data into distinct clusters based on the selected features. the clustering results are ideal, and the B-Cubed precision and recall metrics reflect the high quality of the clustering outcome.
```{r}
# Convert relevant columns to numeric
numeric_dataset <- dataset
numeric_dataset$Age <- as.numeric(as.character(numeric_dataset$Age))
numeric_dataset$Survival.Months <- as.numeric(as.character(numeric_dataset$Survival.Months))

# Check the structure of numeric_dataset
str(numeric_dataset)

# Define k_medoids_clustering function
k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 4  # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_assignments)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_codes)

# Corrected B-Cubed metrics calculation
bcubed_precision_recall <- function(labels_true, labels_pred) {
  labels_true <- as.factor(labels_true)
  labels_pred <- as.factor(labels_pred)
  
  unique_labels <- unique(labels_true)
  num_points <- length(labels_true)
  
  precision_sum <- 0
  recall_sum <- 0
  
  for (true_label in unique_labels) {
    true_indices <- labels_true == true_label
    pred_labels <- labels_pred[true_indices]
    
    unique_pred_labels <- unique(pred_labels)
    
    for (pred_label in unique_pred_labels) {
      common_points <- sum(pred_labels == pred_label)
      precision_sum <- precision_sum + common_points / sum(labels_pred == pred_label)
      recall_sum <- recall_sum + common_points / sum(labels_true == true_label)
    }
  }
  
  precision <- precision_sum / num_points
  recall <- recall_sum / num_points
  
  return(list(precision = precision, recall = recall))
}

precision_recall <- bcubed_precision_recall(selected_data$Cluster, cluster_codes)
precision <- precision_recall$precision
recall <- precision_recall$recall

print(paste("B-Cubed Precision:", precision))
print(paste("B-Cubed Recall:", recall))

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_codes)

# Calculate dissimilarity matrix with integer row names
diss_matrix <- as.dist(dist(selected_data))
rownames(diss_matrix) <- as.integer(rownames(diss_matrix))

# Calculate Silhouette score
silhouette_avg <- silhouette(diss_matrix, cluster_codes)$avg.width

# Print the average Silhouette score
print(paste("Average Silhouette:", silhouette_avg))

# Create scatter plot with colored clusters
library(ggplot2)

ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")



```

#here you can run the cluster with k=5 and see its graph and b-cubed results
```{r}
# Convert relevant columns to numeric
numeric_dataset <- dataset
numeric_dataset$Age <- as.numeric(as.character(numeric_dataset$Age))
numeric_dataset$Survival.Months <- as.numeric(as.character(numeric_dataset$Survival.Months))

# Check the structure of numeric_dataset
str(numeric_dataset)

# Define k_medoids_clustering function
k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 5  # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_assignments)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_codes)

# Corrected B-Cubed metrics calculation
bcubed_precision_recall <- function(labels_true, labels_pred) {
  labels_true <- as.factor(labels_true)
  labels_pred <- as.factor(labels_pred)
  
  unique_labels <- unique(labels_true)
  num_points <- length(labels_true)
  
  precision_sum <- 0
  recall_sum <- 0
  
  for (true_label in unique_labels) {
    true_indices <- labels_true == true_label
    pred_labels <- labels_pred[true_indices]
    
    unique_pred_labels <- unique(pred_labels)
    
    for (pred_label in unique_pred_labels) {
      common_points <- sum(pred_labels == pred_label)
      precision_sum <- precision_sum + common_points / sum(labels_pred == pred_label)
      recall_sum <- recall_sum + common_points / sum(labels_true == true_label)
    }
  }
  
  precision <- precision_sum / num_points
  recall <- recall_sum / num_points
  
  return(list(precision = precision, recall = recall))
}

precision_recall <- bcubed_precision_recall(selected_data$Cluster, cluster_codes)
precision <- precision_recall$precision
recall <- precision_recall$recall

print(paste("B-Cubed Precision:", precision))
print(paste("B-Cubed Recall:", recall))

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_codes)

# Calculate dissimilarity matrix with integer row names
diss_matrix <- as.dist(dist(selected_data))
rownames(diss_matrix) <- as.integer(rownames(diss_matrix))

# Calculate Silhouette score
silhouette_avg <- silhouette(diss_matrix, cluster_codes)$avg.width

# Print the average Silhouette score
print(paste("Average Silhouette:", silhouette_avg))

# Create scatter plot with colored clusters
library(ggplot2)

ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")

  


```

#here you can run the cluster with k=6 and see its graph and b-cubed results
```{r}
# Convert relevant columns to numeric
numeric_dataset <- dataset
numeric_dataset$Age <- as.numeric(as.character(numeric_dataset$Age))
numeric_dataset$Survival.Months <- as.numeric(as.character(numeric_dataset$Survival.Months))

# Check the structure of numeric_dataset
str(numeric_dataset)

# Define k_medoids_clustering function
k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 6  # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_assignments)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_codes)

# Corrected B-Cubed metrics calculation
bcubed_precision_recall <- function(labels_true, labels_pred) {
  labels_true <- as.factor(labels_true)
  labels_pred <- as.factor(labels_pred)
  
  unique_labels <- unique(labels_true)
  num_points <- length(labels_true)
  
  precision_sum <- 0
  recall_sum <- 0
  
  for (true_label in unique_labels) {
    true_indices <- labels_true == true_label
    pred_labels <- labels_pred[true_indices]
    
    unique_pred_labels <- unique(pred_labels)
    
    for (pred_label in unique_pred_labels) {
      common_points <- sum(pred_labels == pred_label)
      precision_sum <- precision_sum + common_points / sum(labels_pred == pred_label)
      recall_sum <- recall_sum + common_points / sum(labels_true == true_label)
    }
  }
  
  precision <- precision_sum / num_points
  recall <- recall_sum / num_points
  
  return(list(precision = precision, recall = recall))
}

precision_recall <- bcubed_precision_recall(selected_data$Cluster, cluster_codes)
precision <- precision_recall$precision
recall <- precision_recall$recall

print(paste("B-Cubed Precision:", precision))
print(paste("B-Cubed Recall:", recall))

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_codes)

# Calculate dissimilarity matrix with integer row names
diss_matrix <- as.dist(dist(selected_data))
rownames(diss_matrix) <- as.integer(rownames(diss_matrix))

# Calculate Silhouette score
silhouette_avg <- silhouette(diss_matrix, cluster_codes)$avg.width

# Print the average Silhouette score
print(paste("Average Silhouette:", silhouette_avg))

# Create scatter plot with colored clusters
library(ggplot2)

ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")



```
