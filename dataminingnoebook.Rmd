---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


#importing dataset
```{r}
setwd("C:/Users/saran/OneDrive")
dataset = read.csv('BreastCancer1.csv')
str(dataset)
library(Hmisc)
describe(dataset)
summary(dataset)
```
#histogram for ages
```{r}
hist(dataset$Age)
```
#A histogram showing the age distribution of women diagnosed with breast cancer. The histogram shows that breast cancer is more common among women whose ages range from 45 t0 50 years(highest frequency).

#pie chart for race
```{r}
library(dplyr)
tab = dataset$Race %>% table()
precentages = tab %>% prop.table() %>% round(3)*100
txt = paste0(names(tab), '\n', precentages, '%')
pie(tab, labels = txt, main= 'Race')
```
#The pie chat divided the races into three sections with their percentage ( as our dataset shows we have white, black , other). As we see, the white race preset the highest percentage ( 84.8%).




#Scatter Plot
```{r}
with(dataset, plot(Tumor.Size, Survival.Months, col = "blue", pch = 16))
#This scatter plot has no correlation although we can see that when the tumor size is small the human is more likely to live more months which make sense
```




#changing the value IV in Grade attribute
```{r}
dataset$Grade[dataset$Grade == ' anaplastic; Grade IV' ] = 4 
table(dataset$Grade)
```
#install.packages("outliers")
```{r}
library(outliers)
#outliers of age
outage= outlier(dataset$Age)
print(outage)
#number of rows that have the outlier 
table(dataset$Age == outage )

#outliers of tumor size
outTumorSize = outlier(dataset$Tumor.Size)
print(outTumorSize)
#number of rows that has the outlier
table(dataset$Tumor.Size == outTumorSize)

#outliers of Regional Node Examined
outNodeExamined = outlier(dataset$Regional.Node.Examined)
print(outNodeExamined)
#number of rows that has the outlier
table(dataset$Regional.Node.Examined == outNodeExamined )

#outliers of Reginol Node Positive
outNodepositive = outlier(dataset$Reginol.Node.Positive)
print(outNodepositive)
#number of rows that has the outlier
table(dataset$Reginol.Node.Positive == outNodepositive )

#outliers of Survival Months
outmonth = outlier(dataset$Survival.Months)
print(outmonth)
#number of rows that has the outlier 
table(dataset$Survival.Months==outmonth)
```

```{r}
#delete the outliers
dataset = dataset[dataset$Age != outage ,]
dataset = dataset[dataset$Tumor.Size != outTumorSize ,]
dataset = dataset[dataset$Regional.Node.Examined != outNodeExamined ,]
dataset = dataset[dataset$Reginol.Node.Positive != outNodepositive ,]
dataset = dataset[dataset$Survival.Months != outmonth ,]
nrow(dataset)
```
#checking if there is any missing value or duplicated rows 
is.na(dataset)
sum(is.na(dataset))
sum(duplicated(dataset))
#removing duplicate
dataset= unique(dataset)
sum(duplicated(dataset))
#number of rows after deleting dup
nrow(dataset)
nrow(dataset)

#Normalization:
#Define function normalize
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}

#Call normalize function
dataset$Age<-normalize(dataset$Age)
dataset$Tumor.Size<-normalize(dataset$Tumor.Size)
dataset$Regional.Node.Examined<-normalize(dataset$Regional.Node.Examined)
dataset$Reginol.Node.Positive<-normalize(dataset$Reginol.Node.Positive)
dataset$Survival.Months<-normalize(dataset$Survival.Months)


#showing normalized columns
print(dataset)

#encoding
dataset$Race = factor(dataset$Race,levels = c("White","Black", "Other"), labels=c(1,2,3))
dataset$Marital.Status = factor(dataset$Marital.Status,levels = c("Married","Divorced","Separated","Single ","Widowed"), labels=c(1,2,3,4,5))
dataset$differentiate = factor(dataset$differentiate,levels = c("Well differentiated","Moderately differentiated", "Poorly differentiated","Undifferentiated"), labels=c(1,2,3,4))
dataset$X6th.Stage = factor(dataset$X6th.Stage,levels = c("IIA","IIIA", "IIB","IIIB","IIIC"), labels=c(1,2,3,4,5))
dataset$Status = factor(dataset$Status,levels = c("Dead","Alive"), labels=c(0,1))
dataset$Estrogen.Status = factor(dataset$Estrogen.Status ,levels = c("Negative","Positive"), labels=c(0,1))
dataset$Progesterone.Status = factor(dataset$Progesterone.Status ,levels = c("Negative","Positive"), labels=c(0,1))
dataset$A.Stage	 = factor(dataset$A.Stage	,levels = c("Regional","Distant"), labels=c(0,1))
dataset$T.Stage	 = factor(dataset$T.Stage	,levels = c("T1","T2","T3","T4"), labels=c(1,2,3,4))
dataset$N.Stage	 = factor(dataset$N.Stage	,levels = c("N1","N2","N3"), labels=c(1,2,3))

is.na(dataset)
sum(is.na(dataset))

#feature selection
# Get the column names of the dataset
column_names <- colnames(dataset)

# Perform chi-square test for each attribute
for (attribute in column_names) {
  if (attribute != "Status") {
    # Create a contingency table
    contingency_table <- table(dataset[[attribute]], dataset$Status)
    
    # Perform chi-square test
    result <- chisq.test(contingency_table)
    
    # Print the attribute name and the result
    cat("Attribute:", attribute, "\n")
    print(result)
    cat("\n")
  }
}
#the chi-square test results indicate that most of the attributes have a significant association with the variable being tested. These attributes include Age, Race, Marital.Status, T.Stage, N.Stage, X6th.Stage, differentiate, Grade, A.Stage, Estrogen.Status, Progesterone.Status, Regional.Node.Examined, Reginol.Node.Positive, Tumor.Size, Survival.Months, and Status. The low p-values provide strong evidence to reject the null hypothesis of no association between these attributes and the variable.
#Since they are highly correlated there is no need to perform feature selection






#other way to choose the relevant attributes to the dataset, we performed this feature selection if you want to check it out, although we think the chi square is better
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# Split the dataset into features and target variable
features <- dataset[, -c(2 , 3 ,16)]  # Exclude non-numeric and target variable columns
target <- dataset$Status

# Define the control parameters for RFE
ctrl <- rfeControl(functions = rfFuncs, 
                   method = "cv",
                   number = 10)  # 10-fold cross-validation

# Assuming 'rfFuncs' is correctly defined for Recursive Feature Elimination using Random Forest
rfe_result <- rfe(features, target, sizes = 1:ncol(features), rfeControl = ctrl)

# Print the RFE result
print(rfe_result)
plot(rfe_result)
#This feature selection graphical representation shows the higher-ranked features that considered more important or relevant for predicting the target variable which are: Survival.Months, Reginol.Node.Positive, X6th.Stage, Progesterone. Status, Age.,this helps choosing which features to include or exclude . so we can improve model efficiency, reduce overfitting, and enhance interpretability by focusing on the most relevant features for the task at hand.

#Classification
```{r}
dataset$Status <- factor(dataset$Status)
# Set the random seed for reproducibility
set.seed(1234)

# Split the dataset into training 70% and testing 30% subsets
ind <- sample(2, nrow(dataset), replace = TRUE, prob = c(0.7 , 0.3))
trainData <- dataset[ind == 1,]
testData <- dataset[ind == 2,]
myFormula <- Status ~ Age + Marital.Status + Race + T.Stage + N.Stage + X6th.Stage + differentiate + Grade + A.Stage + Tumor.Size + Estrogen.Status + Progesterone.Status + Reginol.Node.Positive + Regional.Node.Examined + Survival.Months 
```

```{r}
# Load the necessary library
install.packages('party')
library(party)
install.packages("rpart")
install.packages('rpart.plot')
library('rpart.plot')
library(rpart)
#tree 1 for information gain 
# Using ID3 with rpart
dataset.id3 <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
printcp(dataset.id3)  # Display cp table for pruning
```

```{r}
# Plot the decision tree1
rpart.plot(dataset.id3)
```

```{r}
text(dataset.id3)
```

```{r}
# Make predictions on the test set
testPred <- predict(dataset.id3, newdata = testData, type = "class")
result <- table(testPred, testData$Status)
```

```{r}
#Evaluate the model:
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
co_result <- confusionMatrix(result)
print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))
acc <- co_result$overall["Accuracy"]
acc
```
The formula myFormula is used to define the target variable (Status) and predictor variables for the classification.

We made decision tree classification using the ID3 algorithm (information gain) on a dataset then we split the dataset into 70% training and 30% testing. 
For visualization the tree 
In this case, the output lists the variables that are actually used in constructing the classification tree which are
the following variables are used: Age, Estrogen.Status, Reginol.Node.Positive, Regional.Node.Examined, Survival.Months, and Tumor.Size to know the status (0 dead, 1 alive).
Survival months has the highest Information Gain so it’s chosen as the splitting criterion at each node of the decision tree.

For the Predictions , they are made on the test set using the trained decision tree.
we calculated confusion matrix, sensitivity, specificity, precision, and overall accuracy . And the accuracy was high which indicates that a significant portion of the predictions made by the model match the actual outcomes in the test dataset as well as the other measurments.
```{r}
# Using CART with rpart
#tree 2 using gini index
dataset.cart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))
# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#the result
testPredc <- predict(dataset.cart, newdata = testData, type = "class")
resultc <- table(testPredc, testData$Status)
```

```{r}
#Evaluate the model:
co_resultc <- confusionMatrix(resultc)
print(co_resultc)
sensitivity(as.table(co_resultc))
specificity(as.table(co_resultc))
precision(as.table(co_resultc))
accc <- co_resultc$overall["Accuracy"]
accc
```
We made decision tree classification using the CARAT algorithm (gini index) on a dataset then we split the dataset into 70% training and 30% testing. 
For visualization the tree 
In this case, the tree is built using the variables Age, Estrogen.Status, Reginol.Node.Positive, Regional.Node.Examined, and Survival.Months to know the status (0 dead, 1 alive).
Survival months has minimum Gini index so it’s the root.
For the Predictions , they are made on the test set using the trained decision tree.
we calculated confusion matrix, sensitivity, specificity, precision, and overall accuracy . And the accuracy was high which indicates that a significant portion of the predictions made by the model match the actual outcomes in the test dataset.

```{r}
#tree3 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
dataset.c45 <- C5.0(myFormula, data = trainData)
# Print the tree rules
summary(dataset.c45)
```

```{r}
# Make predictions on the test set
testPreds <- predict(dataset.c45, newdata = testData)
results <- table(testPreds, testData$Status)
c45_party <- as.party(dataset.c45)
```

```{r}
# Plot the decision tree
plot(c45_party)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_results <- confusionMatrix(results)
print(co_results)
sensitivity(as.table(co_results))
specificity(as.table(co_results))
precision(as.table(co_results))
accs <- co_results$overall["Accuracy"]
accs

```
We made decision tree classification using the C5.0 algorithm (gain ratio) on a dataset then we split the dataset into 70% training and 30% testing. 
For visualization the tree 
In this case, The most important attribute is "Survival.Months" with 100% usage, followed by "Age," "Estrogen.Status," and "N.Stage.".
The size of the tree is indicated by the number of nodes. In this case, the tree has a size of 5 nodes.
The tree has 236 errors, representing a misclassification rate of 9.8%
For the Predictions , they are made on the test set using the trained decision tree.
we calculated confusion matrix, sensitivity, specificity, precision, and overall accuracy . And the accuracy was high which indicates that a significant portion of the predictions made by the model match the actual outcomes in the test dataset.
```{r}

# Split the dataset into 80% training and 20% testing subsets
indfo <- sample(2, nrow(dataset), replace = TRUE, prob = c(0.8 , 0.2))
trainData <- dataset[indfo == 1,]
testData <- dataset[indfo == 2,]
```

```{r}
# tree4 using information gain
# Using ID3 with rpart
datasetID3 <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
print(datasetID3)  # Display cp table for pruning

# Plot the decision tree
rpart.plot(datasetID3)
text(datasetID3)

```

```{r}
# Make predictions on the test set
testPredec <- predict(datasetID3, newdata = testData, type = "class")
resultdd<- table(testPredec, testData$Status)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
coresult <- confusionMatrix(resultdd)
print(coresult)
sensitivity(as.table(coresult))
specificity(as.table(coresult))
precision(as.table(coresult))
accur<- coresult$overall["Accuracy"]
accur
```
The formula myFormula is used to define the target variable (Status) and predictor variables for the classification.

We made decision tree classification using the ID3 algorithm (information gain) on a dataset then we split the dataset into 80% training and 20% testing. 
For visualization the tree 
In this case, the output lists the variables that are actually used in constructing the classification tree which are
the following variables are used: Age, Estrogen.Status, Reginol.Node.Positive, Survival.Months, and Tumor.Size to know the status (0 dead, 1 alive).
Survival months has the highest Information Gain so it’s chosen as the splitting criterion at each node of the decision tree.

For the Predictions , they are made on the test set using the trained decision tree.
we calculated confusion matrix, sensitivity, specificity, precision, and overall accuracy . And the accuracy was high which indicates that a significant portion of the predictions made by the model match the actual outcomes in the test dataset.
```{r}
#tree5 using gini index
# Using CART with rpart
datasetcart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))

# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#result
testPredcs<- predict(datasetcart, newdata = testData, type = "class")
resultcs <- table(testPredcs, testData$Status)
```

```{r}
#Evaluate the model:
co_resultcg <- confusionMatrix(resultcs)
print(co_resultcg)
sensitivity(as.table(co_resultcg))
specificity(as.table(co_resultcg))
precision(as.table(co_resultcg))
acccu<- co_resultcg$overall["Accuracy"]
acccu
```
We made decision tree classification using the CARAT algorithm (gini index) on a dataset then we split the dataset into 80% training and 20% testing. 
For visualization the tree 
In this case, the tree is built using the variables Age, Estrogen.Status, Reginol.Node.Positive, Regional.Node.Examined, Tumor.size , survival months to know the status (0 dead, 1 alive).
Survival months has minimum Gini index so it’s the root.

For the Predictions , they are made on the test set using the trained decision tree.
we calculated confusion matrix, sensitivity, specificity, precision, and overall accuracy . And the accuracy was high which indicates that a significant portion of the predictions made by the model match the actual outcomes in the test dataset as well as the other measurements

```{r}
#tree6 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
datasetc45 <- C5.0(myFormula, data = trainData)
# Print the tree rules
summary(datasetc45)
```

```{r}
# Make predictions on the test set
testPredsn <- predict(datasetc45, newdata = testData)
resultsn<- table(testPredsn, testData$Status)
c45party <- as.party(datasetc45)

```

```{r}
# Plot the decision tree
plot(c45party)

```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_resultsu <- confusionMatrix(resultsn)
print(co_resultsu)
sensitivity(as.table(co_resultsu))
specificity(as.table(co_resultsu))

accsn <- co_resultsu$overall["Accuracy"]
accsn
```
The following is a 20% testing and 80% Training information gain ratio created where the classification label is known (pre-classified) for each record. Next, the algorithm systematically assigns each record to one of two subsets on the some basis ( Survival.Month > 0.429 or Survival.Month <=0.429). The object is to attain an homogeneous set of labels in each partitionThis partitioning (splitting) is then applied to each of the new partitions. The process continues until no more useful splits can be found. 
The goal is to build a tree that distinguishes among the classes. For sensitivity, sensitivity, accuracy, precision and a lot of other measures. as we can see below the sensitivity of the tree says its 42% correctly identified by the classifier which is not a really perfect thing but the rest of the measures were great 
```{r}
# Split the dataset into 60% training and 40% testing subsets
indfod<- sample(2, nrow(dataset), replace = TRUE, prob = c(0.6 , 0.4))
trainData <- dataset[indfod== 1,]
testData <- dataset[indfod == 2,]
```

```{r}
#tree 7 using information gain
# Using ID3 with rpart
datasetID3.<- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
print(datasetID3)  # Display cp table for pruning

```

```{r}
# Plot the decision tree
rpart.plot(datasetID3.)
text(datasetID3.)
```

```{r}
# Make predictions on the test set
testPredecy<- predict(datasetID3, newdata = testData, type = "class")
resultddy<- table(testPredecy, testData$Status)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

coresulty <- confusionMatrix(resultddy)
print(coresulty)
sensitivity(as.table(coresulty))
specificity(as.table(coresulty))
precision(as.table(coresulty))
accury<- coresulty$overall["Accuracy"]
accury
```
 A 60% Training Set is created where the classification label is known (pre-classified) for each record. Next, the algorithm systematically assigns each record to one of two subsets on the some basis whether ( Survival.Month <0.43) or not . The object is to attain an homogeneous set of labels in each partitionThis partitioning (splitting) is then applied to each of the new partitions. The process continues until no more useful splits can be found. 
The goal is to build a tree that distinguishes among the classes. For sensitivity, sensitivity, accuracy, precision and a lot of other measures. as we can see below the sensitivity of the tree says its 49% correctly identified by the classifier which is not a really perfect thing but the rest of the measures were great 
```{r}
#tree 8 using gini index
# Using CART with rpart
datasetcart. <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))

# Plot the decision tree
library(rpart.plot)
rpart.plot(dataset.cart)
```

```{r}
#result

testPredcse<- predict(datasetcart., newdata = testData, type = "class")
resultcse <- table(testPredcse, testData$Status)
```

```{r}
#Evaluate the model:
co_resultcge <- confusionMatrix(resultcse)
print(co_resultcge)
sensitivity(as.table(co_resultcge))
specificity(as.table(co_resultcge))
precision(as.table(co_resultcge))
acccue<- co_resultcge$overall["Accuracy"]
acccue
```
 The following is a 40% testing and 60% Training gini index created where the classification label is known (pre-classified) for each record. Next, the algorithm systematically assigns each record to one of two subsets on the some basis whether ( Survival.Month <0.43) or not . The object is to attain an homogeneous set of labels in each partitionThis partitioning (splitting) is then applied to each of the new partitions. The process continues until no more useful splits can be found. 
The goal is to build a tree that distinguishes among the classes. For sensitivity, sensitivity, accuracy, precision and a lot of other measures. as we can see below the sensitivity of the tree says its 48% correctly identified by the classifier which is not a really perfect thing but the rest of the measures were great 
```{r}
#tree 9 using gain ratio
# Install and load the C50 package if not already installed
# install.packages("C50")
library(C50)
library(partykit)
library(party)
# Using C4.5 (C5.0) with C50
datasetc45. <- C5.0(myFormula, data = trainData)

# Print the tree rules
summary(datasetc45.)
```

```{r}

# Make predictions on the test set
testPredsni <- predict(datasetc45., newdata = testData)
resultsni<- table(testPredsni, testData$Status)
c45party. <- as.party(datasetc45.)
```

```{r}
# Plot the decision tree
plot(c45party.)
```

```{r}
#Evaluate the model:
#install.packages("caret")
#install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)
co_resultsui <- confusionMatrix(resultsni)
print(co_resultsui)
sensitivity(as.table(co_resultsui))
specificity(as.table(co_resultsui))
precision(as.table(co_resultsui))
accsni <- co_resultsui$overall["Accuracy"]
accsni
```
 The following is a 40% testing and 60% Training information gain ratio created where the classification label is known (pre-classified) for each record. Next, the algorithm systematically assigns each record to one of two subsets on the some basis whether ( Survival.Month > 0.429 or Survival.Month <=0.429).not . The object is to attain an homogeneous set of labels in each partitionThis partitioning (splitting) is then applied to each of the new partitions. The process continues until no more useful splits can be found. 
The goal is to build a tree that distinguishes among the classes. For sensitivity, sensitivity, accuracy, precision and a lot of other measures. as we can see below the sensitivity of the tree says its 45% correctly identified by the classifier which is not a really perfect thing but the rest of the measures were great 

In summary, the three partitions (0.7 , 0.3 and 0.6 , 0.4 and 0.8 , 0.2) and the three attributes selection (IG, IG ratio, gini index)  lead to similar results , such as all of them have an accuracy above 0.9 and other measurements have similar results.
This is a positive sign indicating that the model's performance is consistent and not overly dependent on specific choices.
Information Gain, Information Gain Ratio, and Gini Index algorithms lead to similar results, it indicates that, these metrics are providing comparable insights into the dataset.
consistency across different partitions and attribute selection methods is a positive signal, it's advisable to perform thorough evaluation, including testing on a separate dataset, to ensure the model's reliability and generalization capabilities

#Clustring
#For clustering I chose k-medioid method because it the best fit with my dataset, I tried three Ks 4,5,6.
By my understanding I found that 4 is the best K because the points well
distributed. I chose the attributes survival months and age after trying multiple attributes i found they give the best results.

The resulting plot displays a scatter plot where each data point is represented as a point on the plot. The points are colored based on their assigned cluster, allowing for visual exploration of the clustering results. The plot title and axis labels provide additional context for interpreting the plot, this plot shows 4 clusters for this dataset and is chosen as the optimal k number

#Clustr of size=4
#this number was chosen after applying the elbow method which shows that 5 is the optimal number for clustring
```{r}
#Converting interger&factor columns to numeric SO we can perform k-means
numeric_dataset = dataset
numeric_dataset$Age = as.numeric(as.character(dataset$Age))
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Tumor.Size = as.numeric(as.character(dataset$Tumor.Size))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
numeric_dataset$Regional.Node.Examined = as.numeric(as.character(dataset$Regional.Node.Examined))
numeric_dataset$Reginol.Node.Positive = as.numeric(as.character(dataset$Reginol.Node.Positive))
numeric_dataset$Survival.Months = as.numeric(as.character(dataset$Survival.Months))
numeric_dataset$Status = as.numeric(as.character(dataset$Status))
str(numeric_dataset)
library(cluster)
library(ggplot2)
# Assuming 'data' is your object
if (class(data) == "data.frame") {
  # Your code for subsetting columns here
} else {
  # Handle the case where 'data' is not a data frame
  print("The 'data' object is not a data frame.")
}

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}
# Example usage
selected_columns <- c('Age', "Survival.Months")  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 4 # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_assignments)


# Create scatter plot with colored clusters
ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")
```

#Clustr of size=5
#this number was chosen randomlly
```{r}
#Converting interger&factor columns to numeric SO we can perform k-means
numeric_dataset = dataset
numeric_dataset$Age = as.numeric(as.character(dataset$Age))
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Tumor.Size = as.numeric(as.character(dataset$Tumor.Size))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
numeric_dataset$Regional.Node.Examined = as.numeric(as.character(dataset$Regional.Node.Examined))
numeric_dataset$Reginol.Node.Positive = as.numeric(as.character(dataset$Reginol.Node.Positive))
numeric_dataset$Survival.Months = as.numeric(as.character(dataset$Survival.Months))
numeric_dataset$Status = as.numeric(as.character(dataset$Status))
str(numeric_dataset)
library(cluster)
library(ggplot2)
# Assuming 'data' is your object
if (class(data) == "data.frame") {
  # Your code for subsetting columns here
} else {
  # Handle the case where 'data' is not a data frame
  print("The 'data' object is not a data frame.")
}

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}
# Example usage
selected_columns <- c('Age', "Survival.Months")  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 5 # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_assignments)


# Create scatter plot with colored clusters
ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")
```




#Clustr of size=6
#this number was chosen after applying Silhouette coefficient which indicate 6 as the optimal k
```{r}
#Converting interger&factor columns to numeric SO we can perform k-means
numeric_dataset = dataset
numeric_dataset$Age = as.numeric(as.character(dataset$Age))
numeric_dataset$Race = as.numeric(as.character(dataset$Race))
numeric_dataset$Marital.Status = as.numeric(as.character(dataset$Marital.Status))
numeric_dataset$T.Stage = as.numeric(as.character(dataset$T.Stage))
numeric_dataset$N.Stage = as.numeric(as.character(dataset$N.Stage))
numeric_dataset$X6th.Stage = as.numeric(as.character(dataset$X6th.Stage))
numeric_dataset$differentiate = as.numeric(as.character(dataset$differentiate))
numeric_dataset$Grade = as.numeric(as.character(dataset$Grade))
numeric_dataset$A.Stage = as.numeric(as.character(dataset$A.Stage))
numeric_dataset$Tumor.Size = as.numeric(as.character(dataset$Tumor.Size))
numeric_dataset$Estrogen.Status = as.numeric(as.character(dataset$Estrogen.Status))
numeric_dataset$Progesterone.Status = as.numeric(as.character(dataset$Progesterone.Status))
numeric_dataset$Regional.Node.Examined = as.numeric(as.character(dataset$Regional.Node.Examined))
numeric_dataset$Reginol.Node.Positive = as.numeric(as.character(dataset$Reginol.Node.Positive))
numeric_dataset$Survival.Months = as.numeric(as.character(dataset$Survival.Months))
numeric_dataset$Status = as.numeric(as.character(dataset$Status))
str(numeric_dataset)


library(cluster)
library(ggplot2)
# Assuming 'data' is your object
if (class(data) == "data.frame") {
  # Your code for subsetting columns here
} else {
  # Handle the case where 'data' is not a data frame
  print("The 'data' object is not a data frame.")
}

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}
# Example usage
selected_columns <- c('Age', "Survival.Months")  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 6 # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_assignments)


# Create scatter plot with colored clusters
ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")
```
#The elbow method is a technique used to determine the optimal number of clusters in a clustering analysis by looking for the "elbow" point in the plot, which is the point of diminishing returns in reducing the within-cluster sum of squares (WSS) as the number of clusters increases. The plot helps visualize this relationship and assists in determining an appropriate number of clusters for the given data. which is 4 in our dataset
#also choosing the optimal number of clusters can help represinting the data in the best way possible and saves much time

```{r}
library(cluster)
library(ggplot2)

k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
max_k <- 10  # Maximum number of clusters to consider

wss_values <- vector("numeric", max_k)  # Empty vector to store WSS values

for (k in 1:max_k) {
  cluster_assignments <- k_medoids_clustering(selected_data, k)  # Perform k-medoids clustering
  wss <- 0  # Initialize within-cluster sum of squares (WSS) value
  
  # Calculate WSS for the current k
  for (i in 1:k) {
    cluster_points <- selected_data[cluster_assignments == i, , drop = FALSE]
    
    # Calculate medoid as the observation with minimum average dissimilarity
    cluster_distances <- proxy::dist(cluster_points)
    avg_dissimilarity <- rowMeans(as.matrix(cluster_distances))
    medoid_index <- which.min(avg_dissimilarity)
    medoid <- cluster_points[medoid_index, ]
    
    cluster_distances <- proxy::dist(rbind(cluster_points, medoid))
    wss <- wss + sum(cluster_distances[1:nrow(cluster_points)])
  }
  
  wss_values[k] <- wss  # Store WSS value
}

# Plot WSS values against k values
elbow_plot <- ggplot() +
  geom_line(aes(x = 1:max_k, y = wss_values)) +
  geom_point(aes(x = 1:max_k, y = wss_values)) +
  labs(title = "Elbow Method", x = "Number of Clusters (k)", y = "Within-Cluster Sum of Squares (WSS)")

print(elbow_plot)
```
#another code for elbow method
```{r}
# 3- Elbow method
#fviz_nbclust() with within cluster sums of squares (wss) method
library(factoextra) 
fviz_nbclust(dataset, pam, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```
The resulting plot helps visualize the trend in silhouette scores as the number of clusters (k) increases. It allows  to identify the optimal number of clusters based on the highest silhouette score, which indicates better clustering quality and separation of data points within their assigned clusters. here we have the optimal k=6
```{r}
# Perform scaling on the data
scaled_data <- scale(data)

# Required libraries
library(fpc)
library(ggplot2)

# Determine the optimal number of clusters using the silhouette method
sil_scores <- c()
for (k in 2:10) {
  kmedoids_result <- pam(scaled_data, k = k)
  dissimilarity <- as.dist(dist(scaled_data))
  sil_scores[k-1] <- cluster.stats(dissimilarity, kmedoids_result$clustering)$avg.silwidth
}

# Plot the silhouette scores
k_values <- 2:10
silhouette_df <- data.frame(k = k_values, silhouette_score = sil_scores)
ggplot(silhouette_df, aes(x = k, y = silhouette_score)) +
  geom_line() +
  geom_point() +
  labs(x = "Number of Clusters (k)", y = "Silhouette Score") +
  ggtitle("Silhouette Score vs. Number of Clusters")

```
#here you can run the cluster and see BCubed precision and recall result thet we performed on the clustering method with k=4, "B-Cubed Precision: 0.00150338261087447""B-Cubed Recall: 0.00150338261087447" #The results are similar It could that when the clusters are well-separated and distinct, a clustering algorithm might assign instances to clusters with a high degree of accuracy, leading to similar precision and recall values
```{r}
# Convert relevant columns to numeric
numeric_dataset <- dataset
numeric_dataset$Age <- as.numeric(as.character(numeric_dataset$Age))
numeric_dataset$Survival.Months <- as.numeric(as.character(numeric_dataset$Survival.Months))

# Check the structure of numeric_dataset
str(numeric_dataset)

# Define k_medoids_clustering function
k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 4  # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_assignments)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_codes)

# Corrected B-Cubed metrics calculation
bcubed_precision_recall <- function(labels_true, labels_pred) {
  labels_true <- as.factor(labels_true)
  labels_pred <- as.factor(labels_pred)
  
  unique_labels <- unique(labels_true)
  num_points <- length(labels_true)
  
  precision_sum <- 0
  recall_sum <- 0
  
  for (true_label in unique_labels) {
    true_indices <- labels_true == true_label
    pred_labels <- labels_pred[true_indices]
    
    unique_pred_labels <- unique(pred_labels)
    
    for (pred_label in unique_pred_labels) {
      common_points <- sum(pred_labels == pred_label)
      precision_sum <- precision_sum + common_points / sum(labels_pred == pred_label)
      recall_sum <- recall_sum + common_points / sum(labels_true == true_label)
    }
  }
  
  precision <- precision_sum / num_points
  recall <- recall_sum / num_points
  
  return(list(precision = precision, recall = recall))
}

precision_recall <- bcubed_precision_recall(selected_data$Cluster, cluster_codes)
precision <- precision_recall$precision
recall <- precision_recall$recall

print(paste("B-Cubed Precision:", precision))
print(paste("B-Cubed Recall:", recall))

# Define function to compute average silhouette for k clusters using silhouette()
silhouette_score <- function(data, k) {
  cluster_assignments <- k_medoids_clustering(data, k)
  ss <- silhouette(cluster_assignments, dist(data))
  sil <- mean(ss[, 3])
  return(sil)
}

# k cluster range from 2 to 10
k <- 2:10

# Call function for each k value
avg_sil <- sapply(k, function(k_val) silhouette_score(selected_data[, c('Age', 'Survival.Months')], k_val))

# Plot the results
plot(k, avg_sil, type = 'b', xlab = 'Number of clusters', ylab = 'Average Silhouette Scores', frame = FALSE)


# Create scatter plot with colored clusters
library(ggplot2)

ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")



```

#here you can run the cluster with k=5 and see its graph and b-cubed results
```{r}
# Convert relevant columns to numeric
numeric_dataset <- dataset
numeric_dataset$Age <- as.numeric(as.character(numeric_dataset$Age))
numeric_dataset$Survival.Months <- as.numeric(as.character(numeric_dataset$Survival.Months))

# Check the structure of numeric_dataset
str(numeric_dataset)

# Define k_medoids_clustering function
k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 5  # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_assignments)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_codes)

# Corrected B-Cubed metrics calculation
bcubed_precision_recall <- function(labels_true, labels_pred) {
  labels_true <- as.factor(labels_true)
  labels_pred <- as.factor(labels_pred)
  
  unique_labels <- unique(labels_true)
  num_points <- length(labels_true)
  
  precision_sum <- 0
  recall_sum <- 0
  
  for (true_label in unique_labels) {
    true_indices <- labels_true == true_label
    pred_labels <- labels_pred[true_indices]
    
    unique_pred_labels <- unique(pred_labels)
    
    for (pred_label in unique_pred_labels) {
      common_points <- sum(pred_labels == pred_label)
      precision_sum <- precision_sum + common_points / sum(labels_pred == pred_label)
      recall_sum <- recall_sum + common_points / sum(labels_true == true_label)
    }
  }
  
  precision <- precision_sum / num_points
  recall <- recall_sum / num_points
  
  return(list(precision = precision, recall = recall))
}

precision_recall <- bcubed_precision_recall(selected_data$Cluster, cluster_codes)
precision <- precision_recall$precision
recall <- precision_recall$recall

print(paste("B-Cubed Precision:", precision))
print(paste("B-Cubed Recall:", recall))

# Define function to compute average silhouette for k clusters using silhouette()
silhouette_score <- function(data, k) {
  cluster_assignments <- k_medoids_clustering(data, k)
  ss <- silhouette(cluster_assignments, dist(data))
  sil <- mean(ss[, 3])
  return(sil)
}

# k cluster range from 2 to 10
k <- 2:10

# Call function for each k value
avg_sil <- sapply(k, function(k_val) silhouette_score(selected_data[, c('Age', 'Survival.Months')], k_val))

# Plot the results
plot(k, avg_sil, type = 'b', xlab = 'Number of clusters', ylab = 'Average Silhouette Scores', frame = FALSE)


# Create scatter plot with colored clusters
library(ggplot2)

ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")

  


```

#here you can run the cluster with k=6 and see its graph and b-cubed results
#"B-Cubed Precision: 0.00150338261087447"
#"B-Cubed Recall: 0.00150338261087447"
#the result is concedred very low
```{r}
# Convert relevant columns to numeric
numeric_dataset <- dataset
numeric_dataset$Age <- as.numeric(as.character(numeric_dataset$Age))
numeric_dataset$Survival.Months <- as.numeric(as.character(numeric_dataset$Survival.Months))

# Check the structure of numeric_dataset
str(numeric_dataset)

# Define k_medoids_clustering function
k_medoids_clustering <- function(data, k) {
  dissimilarity_matrix <- dist(data)  # Compute dissimilarity matrix
  k_medoids_result <- pam(dissimilarity_matrix, k)  # Perform k-medoids clustering
  return(k_medoids_result$clustering)  # Return cluster assignments
}

# Example usage
selected_columns <- c('Age', 'Survival.Months')  # Select relevant columns
selected_data <- numeric_dataset[, selected_columns]
k <- 6  # Number of clusters
cluster_assignments <- k_medoids_clustering(selected_data, k)

# Convert cluster assignments to integer codes
cluster_codes <- as.integer(cluster_assignments)

# Add cluster assignments to the selected data
selected_data$Cluster <- as.factor(cluster_codes)

# Corrected B-Cubed metrics calculation
bcubed_precision_recall <- function(labels_true, labels_pred) {
  labels_true <- as.factor(labels_true)
  labels_pred <- as.factor(labels_pred)
  
  unique_labels <- unique(labels_true)
  num_points <- length(labels_true)
  
  precision_sum <- 0
  recall_sum <- 0
  
  for (true_label in unique_labels) {
    true_indices <- labels_true == true_label
    pred_labels <- labels_pred[true_indices]
    
    unique_pred_labels <- unique(pred_labels)
    
    for (pred_label in unique_pred_labels) {
      common_points <- sum(pred_labels == pred_label)
      precision_sum <- precision_sum + common_points / sum(labels_pred == pred_label)
      recall_sum <- recall_sum + common_points / sum(labels_true == true_label)
    }
  }
  
  precision <- precision_sum / num_points
  recall <- recall_sum / num_points
  
  return(list(precision = precision, recall = recall))
}

precision_recall <- bcubed_precision_recall(selected_data$Cluster, cluster_codes)
precision <- precision_recall$precision
recall <- precision_recall$recall

print(paste("B-Cubed Precision:", precision))
print(paste("B-Cubed Recall:", recall))

# Define function to compute average silhouette for k clusters using silhouette()
silhouette_score <- function(data, k) {
  cluster_assignments <- k_medoids_clustering(data, k)
  ss <- silhouette(cluster_assignments, dist(data))
  sil <- mean(ss[, 3])
  return(sil)
}

# k cluster range from 2 to 10
k <- 2:10

# Call function for each k value
avg_sil <- sapply(k, function(k_val) silhouette_score(selected_data[, c('Age', 'Survival.Months')], k_val))

# Plot the results
plot(k, avg_sil, type = 'b', xlab = 'Number of clusters', ylab = 'Average Silhouette Scores', frame = FALSE)


# Create scatter plot with colored clusters
library(ggplot2)

ggplot(selected_data, aes(x = Age, y = Survival.Months, color = Cluster)) +
  geom_point() +
  labs(title = "K-Medoids Clustering", x = "Age", y = "Survival.Months")
```
#by the provided code for each classification and clustering you can definitely tell that the classification is the best fit for our dataset since we have high accuracy= 0.9 unlike with the clustering=0.00150338261087447
